{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46e5294b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "import torch\n",
    "import gc\n",
    "from sae_lens import SAE  # pip install sae-lens\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "torch.set_grad_enabled(False) # avoid blowing up memory\n",
    "# use cuda if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6255f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a small range dataset for addition\n",
    "addition_dataset = []\n",
    "subtraction_dataset = []\n",
    "for num in range(0, 100):\n",
    "    for num2 in range(0, 100):\n",
    "        addition_dataset.append(\"{}+{}\".format(num, num2))\n",
    "        subtraction_dataset.append(\"{}-{}\".format(num, num2))\n",
    "with open(\"addition_dataset.txt\", \"w\") as f:\n",
    "    for item in addition_dataset:\n",
    "        f.write(f\"{item}\\n\") \n",
    "with open(\"subtraction_dataset.txt\", \"w\") as f:\n",
    "    for item in subtraction_dataset:\n",
    "        f.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8ff6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "# Generate random list of numbers a (100 items)\n",
    "a = [random.randint(10_000, 100_000,) for _ in range(100)]\n",
    "\n",
    "random.seed(123)\n",
    "# Generate random list of numbers b (100 items)\n",
    "b = [random.randint(10_000, 100_000) for _ in range(100)]\n",
    "\n",
    "# Create lists for addition and subtraction\n",
    "addition_list = []\n",
    "subtraction_list = []\n",
    "\n",
    "# Create all pairwise combinations\n",
    "for x in a:\n",
    "    for y in b:\n",
    "        addition_list.append(f\"{x}+{y}\")\n",
    "        subtraction_list.append(f\"{x}-{y}\")\n",
    "\n",
    "# Save addition dataset\n",
    "with open(\"addition_dataset_high_range.txt\", \"w\") as f:\n",
    "    for item in addition_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "# Save subtraction dataset\n",
    "with open(\"subtraction_dataset_high_range.txt\", \"w\") as f:\n",
    "    for item in subtraction_list:\n",
    "        f.write(\"%s\\n\" % item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5973ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import sys\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        nvcc_output = subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"utf-8\")\n",
    "        for line in nvcc_output.splitlines():\n",
    "            if \"release\" in line.lower():\n",
    "                return line\n",
    "        return \"CUDA version not found in nvcc output\"\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        return \"nvcc not found or CUDA not installed\"\n",
    "\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            return f\"GPU: {torch.cuda.get_device_name(0)}, CUDA Available: {torch.cuda.is_available()}, CUDA Device Count: {torch.cuda.device_count()}\"\n",
    "        else:\n",
    "            return \"No CUDA-capable GPU detected\"\n",
    "    except Exception as e:\n",
    "        return f\"Error detecting GPU: {str(e)}\"\n",
    "\n",
    "print(\"System Information:\")\n",
    "print(f\"Operating System: {platform.system()} {platform.release()} ({platform.platform()})\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Version: {get_cuda_version()}\")\n",
    "print(f\"GPU Info: {get_gpu_info()}\")\n",
    "try:\n",
    "    import triton\n",
    "    print(f\"Triton Version: {triton.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Triton Version: Not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c89a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Disable PyTorch compilation to avoid Triton dependency\n",
    "os.environ[\"TORCH_NO_CUDA_COMPILATION\"] = \"1\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "# Function to read dataset from file\n",
    "def read_dataset(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Load datasets\n",
    "addition_data = read_dataset(\"addition_dataset_high_range.txt\")\n",
    "subtraction_data = read_dataset(\"subtraction_dataset_high_range.txt\")\n",
    "\n",
    "# Combine datasets and select 10 random examples\n",
    "combined_data = addition_data + subtraction_data\n",
    "random_examples = random.sample(combined_data, 2)\n",
    "\n",
    "# Function to generate response from model\n",
    "def generate_response(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "# Process each example\n",
    "for example in random_examples:\n",
    "    prompt = f\"What is {example}?\"\n",
    "    response = generate_response(prompt)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Output: {response}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc824bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "prompt = \"Explain the theory of relativity in simple terms.\"\n",
    "output = text_generator(prompt, max_new_tokens=100)\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e12a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "del addition_list, subtraction_list\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b2d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open npy file\n",
    "data = np.load(r\"activ_freq\\addition\\layer_20\\active_counts.npy\", allow_pickle=True)\n",
    "data, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991b6cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b039895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Model and Tokenizer ---\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "num_examples = 800 # number of examples to generate\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# --- Prepare Example Batch ---\n",
    "input_file = \"./data/addition.txt\"\n",
    "# load the data\n",
    "with open(input_file, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [line.strip() for line in lines if line.strip()]\n",
    "np.random.seed(42)\n",
    "examples = np.random.choice(lines, num_examples, replace=False).tolist() \n",
    "\n",
    "batch = tokenizer(\n",
    "    examples,\n",
    "    padding=True,          # Pad to longest in batch\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True,\n",
    ")\n",
    "batch = {k: v.to(model.device) for k, v in batch.items()}  # Move to GPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aec7e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Forward Pass ---\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch, output_hidden_states=True)\n",
    "\n",
    "# outputs.hidden_states is a tuple:\n",
    "# (embeddings_output, layer1_output, layer2_output, ..., layerN_output)\n",
    "\n",
    "# We want layer 20\n",
    "layer_idx = 21\n",
    "layer_hidden = outputs.hidden_states[layer_idx]  # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "# --- Find \"=\" token position for each example ---\n",
    "\n",
    "# Find token id for \"=\"\n",
    "equal_token_id = tokenizer.convert_tokens_to_ids(\"=\")\n",
    "\n",
    "# Now find the index of \"=\" for each sequence\n",
    "equal_positions = (batch[\"input_ids\"] == equal_token_id).int().argmax(dim=1)\n",
    "\n",
    "\n",
    "batch_size, seq_len, hidden_size = layer_hidden.shape\n",
    "\n",
    "# Make a batch index tensor\n",
    "batch_indices = torch.arange(batch_size, device=layer_hidden.device)\n",
    "\n",
    "# Extract activations\n",
    "equal_hidden_states = layer_hidden[batch_indices, equal_positions, :]  # (batch_size, hidden_size)\n",
    "\n",
    "print(equal_hidden_states.shape)  # Should be (batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37850fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Find the index of the last meaningful token (non-padding) for each example ---\n",
    "\n",
    "# Get the attention mask (1 for real tokens, 0 for padding)\n",
    "attention_mask = batch[\"attention_mask\"]  # Shape: (batch_size, seq_len)\n",
    "\n",
    "# Find the index of the last non-padding token for each sequence\n",
    "# attention_mask shape: (batch_size, sequence_length)\n",
    "\n",
    "# Step 1: Flip/reverse along dimension 1\n",
    "reversed_mask = (attention_mask == 1).int().flip(dims=[1])\n",
    "\n",
    "# Step 2: Get argmax on the reversed tensor\n",
    "reversed_idx = reversed_mask.argmax(dim=1)\n",
    "\n",
    "# Step 3: Adjust the index because we flipped it\n",
    "last_idx = attention_mask.size(1) - 1 - reversed_idx\n",
    "\n",
    "\n",
    "batch_size, seq_len, hidden_size = layer_hidden.shape\n",
    "\n",
    "# Make a batch index tensor\n",
    "batch_indices = torch.arange(batch_size, device=layer_hidden.device)\n",
    "\n",
    "# Extract activations at the last meaningful token position\n",
    "last_meaningful_hidden_states = layer_hidden[batch_indices, last_idx, :]  # (batch_size, hidden_size)\n",
    "\n",
    "print(last_meaningful_hidden_states.shape)  # Should be (batch_size, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a82839",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming `batch` is your output from tokenizer\n",
    "attention_mask = batch[\"attention_mask\"]    # shape (batch_size, seq_len)\n",
    "input_ids = batch[\"input_ids\"]               # shape (batch_size, seq_len)\n",
    "eos_token_id = tokenizer.eos_token_id        # get the ID of the EOS token\n",
    "\n",
    "# Find the last index where attention_mask == 1 for each row\n",
    "last_indices = attention_mask.size(1) - 1 - torch.argmax(attention_mask.flip(dims=[1]), dim=1)\n",
    "\n",
    "# Now check if input_ids at these positions == eos_token_id\n",
    "last_tokens = input_ids[torch.arange(input_ids.size(0)), last_indices]\n",
    "is_eos = (last_tokens == eos_token_id)\n",
    "\n",
    "print(is_eos.sum())  # Boolean tensor: True if last real token is EOS\n",
    "\n",
    "# Decode the last tokens to see what they are\n",
    "last_tokens_decoded = tokenizer.batch_decode(last_tokens.unsqueeze(1), skip_special_tokens=False)\n",
    "\n",
    "for idx, token in enumerate(last_tokens_decoded[:10]):\n",
    "    print(f\"Sample {idx}: Last token = '{token}'\")\n",
    "    print(f\"Sample: {examples[idx]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a180f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, tokenizer  # Free up memory\n",
    "del outputs, batch, layer_hidden, equal_positions, equal_token_id, batch_indices\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e6e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "#     release = \"gemma-scope-2b-pt-res-canonical\",\n",
    "#     sae_id = \"layer_20/width_16k/canonical\",\n",
    "# )\n",
    "# sae = sae.to(device)\n",
    "# sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "#     release = \"gemma-scope-2b-pt-att-canonical\",\n",
    "#     sae_id = \"layer_20/width_16k/canonical\",\n",
    "# )\n",
    "\n",
    "\n",
    "# sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "#     release = \"gemma-scope-2b-pt-mlp-canonical\",\n",
    "#     sae_id = \"layer_20/width_16k/canonical\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2b13e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-pt-res-canonical\",\n",
    "    sae_id = f\"layer_20/width_16k/canonical\", # 26 layers (25 is the last index)\n",
    ")\n",
    "sae = sae.to(device)\n",
    "\n",
    "sae_acts = sae.encode(last_meaningful_hidden_states) # last_meaningful_hidden_states, equal_hidden_states\n",
    "\n",
    "sae_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c058edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03053c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1e-5  # or whatever you define as \"zero\"\n",
    "\n",
    "sparsity = (torch.abs(sae_acts) < threshold).float().mean()\n",
    "print(f\"Sparsity: {sparsity.item() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef465b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1e-5\n",
    "\n",
    "# Boolean mask: [800, 16384]\n",
    "active = (torch.abs(sae_acts) > threshold)\n",
    "\n",
    "# Sum across the batch dimension (axis=0)\n",
    "active_counts = active.sum(dim=0).cpu().numpy()  # [16384]\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.bar(range(len(active_counts)), active_counts, width=1.0)\n",
    "plt.title(\"Number of times each feature is active\")\n",
    "plt.xlabel(\"Feature index\")\n",
    "plt.ylabel(\"Active count (out of 800)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2341ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(active_counts)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e8e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter features with active count > 600\n",
    "filtered_counts = active_counts[active_counts > 600]\n",
    "filtered_indices = np.arange(len(filtered_counts))\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.bar(filtered_indices, filtered_counts, width=1.0)\n",
    "plt.title(\"Features with Active Count > 600\")\n",
    "plt.xlabel(\"Filtered Feature Index\")\n",
    "plt.ylabel(\"Active Count (out of 800)\")\n",
    "plt.savefig('filtered_feature_active_counts.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc6da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_counts = np.sort(active_counts)[::-1]  # descending order\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(sorted_counts)\n",
    "plt.title(\"Sorted Active Counts per Feature\")\n",
    "plt.xlabel(\"Feature rank (sorted)\")\n",
    "plt.ylabel(\"Active count (out of 800)\")\n",
    "plt.grid(True)\n",
    "plt.xlim(0, 1000)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c6d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_features = sae_acts > 0.01  # (batch_size, num_features)\n",
    "\n",
    "# Check which features are consistently active across all examples\n",
    "# We sum along the batch dimension and check if a feature is active for all examples (sum = batch_size)\n",
    "consistent_features = active_features.sum(dim=0) >= (sae_acts.shape[0]*0.95)  # (num_features)\n",
    "\n",
    "# Get indices of the features that are consistently active\n",
    "consistent_feature_indices = torch.nonzero(consistent_features).squeeze()\n",
    "\n",
    "# Print the consistent feature indices\n",
    "print(f\"Consistent feature indices: {consistent_feature_indices}\")\n",
    "# print(f\"Number of consistent features: {consistent_feature_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fa7d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten to 1D\n",
    "flat_acts = sae_acts.flatten().cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(flat_acts, bins=100, log=True, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Histogram of All SAE Activations (flattened)\")\n",
    "plt.xlabel(\"Activation Value\")\n",
    "plt.ylabel(\"Log Frequency\")\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205189de",
   "metadata": {},
   "outputs": [],
   "source": [
    "(flat_acts > 5).sum(), (flat_acts < 1).sum(), flat_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a9a16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count how many features are > 0.01 for each sample (row)\n",
    "threshold = 0.01\n",
    "sparsity_counts = (sae_acts > threshold).sum(dim=1).cpu().numpy()  # shape: (800,)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.bar(range(len(sparsity_counts)), sparsity_counts, color='slateblue')\n",
    "plt.title(f\"Number of Active Features per Sample (Activation > {threshold})\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Number of Active Features\")\n",
    "# add min, max, mean , and std to the plot\n",
    "min_count = sparsity_counts.min()\n",
    "max_count = sparsity_counts.max()\n",
    "mean_count = sparsity_counts.mean()\n",
    "std_count = sparsity_counts.std()\n",
    "\n",
    "plt.axhline(y=mean_count, color='b', linestyle='--', label=f'Mean: {mean_count:.2f}', alpha=0.7)\n",
    "plt.axhline(y=mean_count + std_count, color='orange', linestyle='--', label=f'Mean + 2 Std: {mean_count + 2*std_count:.2f}',  alpha=0.7)\n",
    "plt.axhline(y=mean_count - std_count, color='purple', linestyle='--', label=f'Mean - 2 Std: {mean_count - 2*std_count:.2f}',  alpha=0.7)\n",
    "# plt.axhline(y=min_count, color='r', linestyle='--', label=f'Min: {min_count}', alpha=0)\n",
    "# plt.axhline(y=max_count, color='g', linestyle='--', label=f'Max: {max_count}',  alpha=0)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ff4fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a threshold (4 is common, adjust based on your activation scale)\n",
    "threshold = 0.01 # activations are either 0 or >7\n",
    "\n",
    "# Compute how many times each dimension is \"active\"\n",
    "freqs = (sae_acts > threshold).sum(dim=0).cpu().numpy()  # shape: (16384,)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(freqs, color='mediumseagreen')\n",
    "plt.title(\"Feature Activation Frequency\")\n",
    "plt.xlabel(\"Latent Dimension Index\")\n",
    "plt.ylabel(f\"Number of Samples with Activation > {threshold}\")\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8f5f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(freqs>100).sum(), (freqs>100).sum()/len(freqs), freqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f60797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure indices are on CPU and as a list\n",
    "indices = consistent_feature_indices.cpu().tolist()\n",
    "\n",
    "# Extract data for the selected indices\n",
    "subset_data = sae_acts[:, indices].cpu().numpy()\n",
    "\n",
    "# Convert to long-form DataFrame for Seaborn\n",
    "df = pd.DataFrame(subset_data, columns=[f\"Feature {i}\" for i in indices])\n",
    "df_melted = df.melt(var_name=\"Dimension\", value_name=\"Activation\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.violinplot(x=\"Dimension\", y=\"Activation\", data=df_melted, palette=\"pastel\")\n",
    "plt.title(\"Activation Distributions for Selected Latent Dimensions\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183df4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average activation per dimension\n",
    "mean_acts = sae_acts.mean(dim=0).cpu().numpy()\n",
    "\n",
    "# Sort values descending\n",
    "sorted_acts = np.sort(mean_acts)[::-1]\n",
    "\n",
    "# Plot top N\n",
    "top_n = 300  # Adjust as needed\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.bar(range(top_n), sorted_acts[:top_n], color='coral')\n",
    "plt.title(f\"Top {top_n} Latent Dimensions by Mean Activation\")\n",
    "plt.xlabel(\"Sorted Latent Dimensions (Descending)\")\n",
    "plt.ylabel(\"Mean Activation Across 800 Samples\")\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea5c760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "html_template = \"https://neuronpedia.org/{}/{}/{}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "\n",
    "def get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"20-gemmascope-res-16k\", feature_idx=0):\n",
    "    return html_template.format(sae_release, sae_id, feature_idx)\n",
    "\n",
    "html = get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"20-gemmascope-res-16k\", feature_idx=10004)\n",
    "IFrame(html, width=1200, height=600)\n",
    "\n",
    "# Find the top consistent features (active 100% of the time)\n",
    "consistent_feature_indices = torch.nonzero(consistent_features).squeeze()\n",
    "\n",
    "# Loop through each consistent feature and generate the corresponding NeuroPedia dashboard HTML\n",
    "dashboard_htmls = [\n",
    "    get_dashboard_html(sae_release=\"gemma-2-2b\", sae_id=\"20-gemmascope-res-16k\", feature_idx=feature_idx.item())\n",
    "    for feature_idx in consistent_feature_indices\n",
    "]\n",
    "\n",
    "# Display all dashboards using IFrames\n",
    "from IPython.display import display\n",
    "\n",
    "# Display each IFrame (dashboard) for the consistent features\n",
    "for html in dashboard_htmls:\n",
    "    print(html)\n",
    "    # display(IFrame(html, width=1200, height=600))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb6b5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72274706",
   "metadata": {},
   "source": [
    "# MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434727f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete old model and inputs if exist\n",
    "del model\n",
    "del inputs\n",
    "\n",
    "# run garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# clear cached memory in CUDA\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581e2c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "\n",
    "\n",
    "input_file = \"./data/addition.txt\"\n",
    "# load the data\n",
    "with open(input_file, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "\n",
    "len(lines) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeec9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 10 thousand lines randomly\n",
    "np.random.seed(42)\n",
    "lines = np.random.choice(lines, 1000, replace=False).tolist() # 1d list: 100 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3345a911",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    device_map='auto',\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        bnb_8bit_compute_dtype=torch.float16,\n",
    "    ),\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef9f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the data\n",
    "inputs = tokenizer(\n",
    "    lines, \n",
    "    return_tensors=\"pt\", \n",
    "    padding=True, \n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=False,\n",
    "    return_offsets_mapping=True  # Important for mapping tokens back to characters\n",
    ").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc8eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(inputs.keys())\n",
    "# print(type(inputs))\n",
    "\n",
    "# print(inputs[\"input_ids\"].shape) # examples * tokens\n",
    "# print(inputs[\"attention_mask\"].shape) # examples * tokens\n",
    "# print(inputs[\"offset_mapping\"].shape) # examples * tokens * 2 (start and end offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c01a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_token_id = tokenizer(\"=\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "print(f\"Equal token id: {equal_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468786d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_positions = (inputs[\"input_ids\"] == equal_token_id).nonzero(as_tuple=False)\n",
    "# equal_positions.shape: [num_equals_found, 2]\n",
    "# Each row: [batch_idx, sequence_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5176162",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "hidden_states = outputs.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e263a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hidden_states))\n",
    "print(type(hidden_states))\n",
    "print(hidden_states[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401de663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete old model and inputs if exist\n",
    "del model\n",
    "del inputs\n",
    "\n",
    "# run garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# clear cached memory in CUDA\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab260ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import SAE  # pip install sae-lens\n",
    "\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-pt-res-canonical\",\n",
    "    sae_id = \"layer_20/width_16k/canonical\",\n",
    ")\n",
    "sae = sae.to(device)\n",
    "# sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "#     release = \"gemma-scope-2b-pt-att-canonical\",\n",
    "#     sae_id = \"layer_20/width_16k/canonical\",\n",
    "# )\n",
    "\n",
    "\n",
    "# sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "#     release = \"gemma-scope-2b-pt-mlp-canonical\",\n",
    "#     sae_id = \"layer_20/width_16k/canonical\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6eebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = outputs.hidden_states\n",
    "hidden_states = hidden_states[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413ce779",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_acts = sae.encode(hidden_states[21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6617bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_acts.shape, hidden_states[21].shape, hidden_states[21].dtype, sae_acts.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a07d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "html_template = \"https://neuronpedia.org/{}/{}/{}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "\n",
    "def get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"20-gemmascope-res-16k\", feature_idx=0):\n",
    "    return html_template.format(sae_release, sae_id, feature_idx)\n",
    "\n",
    "html = get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"20-gemmascope-res-16k\", feature_idx=10004)\n",
    "IFrame(html, width=1200, height=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
