{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "390b66a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets test forward hooks.\n",
    "# load one SAE\n",
    "# load the model\n",
    "# print attributes/architicture of the sae and the model\n",
    "# feed forward one example to the SAE\n",
    "# get the reconstruction of the activations based on one feature\n",
    "# create a hook that replaces the model's activation with the activation - lambda * reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8717ce2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ce9b998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from sae_lens import SAE\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Disable gradients for memory efficiency\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af440f4c",
   "metadata": {},
   "source": [
    "# Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd64a2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f285cc6f571b4edca8ee49b0ca3fb99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # 4-bit quantization config\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    device_map='auto',\n",
    "    # quantization_config=bnb_config,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f645d",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e14bb969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(prompt):\n",
    "    \"\"\"\n",
    "    Count the number of tokens in a prompt and return detailed tokenization info.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input string to tokenize\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (token_count, token_strings, token_ids, formatted_string)\n",
    "            - token_count: Number of tokens\n",
    "            - token_strings: List of individual token strings\n",
    "            - token_ids: List of token IDs\n",
    "            - formatted_string: Human-readable representation with token boundaries\n",
    "    \"\"\"\n",
    "    # Tokenize the prompt\n",
    "    tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    # Get individual token strings\n",
    "    token_strings = []\n",
    "    for token_id in tokens:\n",
    "        token_str = tokenizer.decode([token_id])\n",
    "        token_strings.append(token_str)\n",
    "    \n",
    "    # Create a formatted string showing token boundaries\n",
    "    formatted_parts = []\n",
    "    for i, token_str in enumerate(token_strings):\n",
    "        # Add token boundaries and numbering for clarity\n",
    "        formatted_parts.append(f\"[{i}:'{token_str}']\")\n",
    "    \n",
    "    formatted_string = \" \".join(formatted_parts)\n",
    "    \n",
    "    return token_count, token_strings, tokens, formatted_string\n",
    "\n",
    "# Example usage:\n",
    "def demo_tokenization(prompt):\n",
    "    \"\"\"\n",
    "    Demonstrate tokenization for a given prompt.\n",
    "    \"\"\"\n",
    "    count, strings, ids, formatted = count_tokens(prompt)\n",
    "    \n",
    "    print(f\"Original text: '{prompt}'\")\n",
    "    print(f\"Token count: {count}\")\n",
    "    print(f\"Token IDs: {ids}\")\n",
    "    print(f\"Token strings: {strings}\")\n",
    "    print(f\"Formatted view: {formatted}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# count total number of output tokens in the results\n",
    "def count_total_output_tokens(results):\n",
    "    \"\"\"\n",
    "    Count total number of output tokens in the results.\n",
    "    \n",
    "    Args:\n",
    "        results: List of tuples (problem, answer)\n",
    "    \n",
    "    Returns:\n",
    "        int: Total number of output tokens\n",
    "    \"\"\"\n",
    "    total_tokens = 0\n",
    "    for _, answer in results:\n",
    "        # Count tokens in the answer\n",
    "        count, _, _, _ = count_tokens(answer)\n",
    "        total_tokens += count\n",
    "    \n",
    "    return total_tokens\n",
    "\n",
    "def calc_correct_answer(problem):\n",
    "    \"\"\"\n",
    "    Calculate the correct answer for a given problem.\n",
    "    \n",
    "    Args:\n",
    "        problem: Problem string (e.g., \"2 + 2\")\n",
    "    \n",
    "    Returns:\n",
    "        str: Correct answer as a string\n",
    "    \"\"\"\n",
    "    # Remove any non-numeric characters except for +, -, *, /\n",
    "    clean_problem = re.sub(r'[^\\d\\s\\+\\-\\*/]', '', problem)\n",
    "    \n",
    "    try:\n",
    "        # Evaluate the expression safely\n",
    "        answer = eval(clean_problem)\n",
    "        return str(answer)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating problem '{problem}': {e}\")\n",
    "        return \"ERROR\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68cd9c2",
   "metadata": {},
   "source": [
    "## generation utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2326a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generation(prompts, max_new_tokens=10, batch_size=8):\n",
    "    \"\"\"\n",
    "    Generate responses for multiple prompts in batches with improved memory management.\n",
    "    \"\"\"\n",
    "    all_responses = []\n",
    "    \n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch_prompts = prompts[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize batch\n",
    "        inputs = tokenizer(\n",
    "            batch_prompts, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True,\n",
    "            add_special_tokens=True\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                repetition_penalty=1,\n",
    "                num_beams=1,  \n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                temperature=0,\n",
    "            )\n",
    "            \n",
    "            # Move outputs to CPU immediately and decode\n",
    "            outputs_cpu = outputs.cpu()\n",
    "            \n",
    "            # Decode batch responses\n",
    "            batch_responses = []\n",
    "            for j, output in enumerate(outputs_cpu):\n",
    "                # Get only the new tokens (after the input)\n",
    "                new_tokens = output[len(inputs.input_ids[j]):]\n",
    "                response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "                batch_responses.append(response.strip())\n",
    "            \n",
    "            # Clean up GPU tensors\n",
    "            del outputs\n",
    "            del inputs\n",
    "            torch.cuda.empty_cache()  # Force GPU memory cleanup\n",
    "        \n",
    "        all_responses.extend(batch_responses)\n",
    "        \n",
    "        # Optional: print progress\n",
    "        print(f\"Processed {min(i + batch_size, len(prompts))}/{len(prompts)} prompts\")\n",
    "    \n",
    "    return all_responses\n",
    "\n",
    "def batch_quiz_model(dataset_path, max_new_tokens=10, batch_size=8, start_batch=0, end_batch=None, prefix=\"You are a calculator. Answer immediately: \", postfix=\"\"):\n",
    "    \"\"\"\n",
    "    Quiz the model with improved memory management.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        problems = [line.strip() for line in f.readlines() if line.strip()]\n",
    "    \n",
    "    # Calculate total number of batches\n",
    "    total_batches = (len(problems) + batch_size - 1) // batch_size\n",
    "    \n",
    "    # Set end_batch if not specified\n",
    "    if end_batch is None:\n",
    "        end_batch = total_batches\n",
    "    \n",
    "    # Validate batch indices\n",
    "    if start_batch < 0 or start_batch >= total_batches:\n",
    "        raise ValueError(f\"start_batch {start_batch} is out of range [0, {total_batches})\")\n",
    "    if end_batch < start_batch or end_batch > total_batches:\n",
    "        raise ValueError(f\"end_batch {end_batch} is out of range [{start_batch}, {total_batches}]\")\n",
    "    \n",
    "    # Calculate problem indices for the specified batch range\n",
    "    start_idx = start_batch * batch_size\n",
    "    end_idx = min(end_batch * batch_size, len(problems))\n",
    "    \n",
    "    # Get subset of problems for this run\n",
    "    subset_problems = problems[start_idx:end_idx]\n",
    "    \n",
    "    # Add prefix/postfix to problems\n",
    "    prompts = [prefix + problem + postfix for problem in subset_problems]\n",
    "    \n",
    "    print(f\"Processing batches {start_batch} to {end_batch-1} ({len(subset_problems)} problems) in batches of {batch_size}...\")\n",
    "    print(f\"Total dataset size: {len(problems)} problems ({total_batches} total batches)\")\n",
    "    \n",
    "    # Generate answers in batches\n",
    "    answers = batch_generation(prompts, max_new_tokens=max_new_tokens, batch_size=batch_size)\n",
    "    \n",
    "    # Combine problems with answers - store prompts, not the full dataset\n",
    "    results = list(zip(prompts, answers))\n",
    "    \n",
    "    # Clean up large variables\n",
    "    del subset_problems\n",
    "    del prompts\n",
    "    del answers\n",
    "    torch.cuda.empty_cache()  # Force GPU memory cleanup\n",
    "    return results\n",
    "\n",
    "def save_results(results, output_path, append=True):\n",
    "    \"\"\"\n",
    "    Save results to file with option to append or overwrite.\n",
    "    \n",
    "    Args:\n",
    "        results: List of tuples (problem, answer)\n",
    "        output_path: Path to output file\n",
    "        append: If True, append to existing file. If False, overwrite\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    mode = 'a' if append else 'w'\n",
    "    with open(output_path, mode) as f:\n",
    "        for problem, answer in results:\n",
    "            f.write(f\"{problem}\\t{answer}\\n\")\n",
    "    \n",
    "    print(f\"Results {'appended to' if append else 'saved to'} {output_path}\")\n",
    "\n",
    "def find_optimal_batch_size():\n",
    "    \"\"\"Find the largest batch size that fits in GPU memory\"\"\"\n",
    "    batch_size = 256\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Test with your actual inference function\n",
    "            test_prompts = [\"5+7=\"] * batch_size\n",
    "            _ = batch_generation(test_prompts, max_new_tokens=6, batch_size=batch_size)\n",
    "            batch_size *= 2  # Double until we hit memory limit\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            # Use half of the failed size\n",
    "            optimal_size = batch_size // 4  # Go back to last working size\n",
    "            break\n",
    "    \n",
    "    return optimal_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec347c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_correct_answers(dataset_path, batch_size=64, start_batch=0, end_batch=None,):\n",
    "    \"\"\"\n",
    "    Load the same questions as batch_quiz_model and calculate correct answers.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to the dataset file\n",
    "        batch_size: Number of problems to process simultaneously (same as used in batch_quiz_model)\n",
    "        start_batch: Starting batch index (0-based, same as used in batch_quiz_model)\n",
    "        end_batch: Ending batch index (exclusive, same as used in batch_quiz_model)\n",
    "        prefix: Prefix that was added to each problem in batch_quiz_model\n",
    "        postfix: Postfix that was added to each problem in batch_quiz_model\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples (full_prompt, correct_answer)\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        problems = [line.strip() for line in f.readlines() if line.strip()]\n",
    "    \n",
    "    # Calculate total number of batches\n",
    "    total_batches = (len(problems) + batch_size - 1) // batch_size\n",
    "    \n",
    "    # Set end_batch if not specified\n",
    "    if end_batch is None:\n",
    "        end_batch = total_batches\n",
    "    \n",
    "    # Validate batch indices (same validation as batch_quiz_model)\n",
    "    if start_batch < 0 or start_batch >= total_batches:\n",
    "        raise ValueError(f\"start_batch {start_batch} is out of range [0, {total_batches})\")\n",
    "    if end_batch < start_batch or end_batch > total_batches:\n",
    "        raise ValueError(f\"end_batch {end_batch} is out of range [{start_batch}, {total_batches}]\")\n",
    "    \n",
    "    # Calculate problem indices for the specified batch range (same as batch_quiz_model)\n",
    "    start_idx = start_batch * batch_size\n",
    "    end_idx = min(end_batch * batch_size, len(problems))\n",
    "    \n",
    "    # Get subset of problems for this run\n",
    "    subset_problems = problems[start_idx:end_idx]\n",
    "    \n",
    "    results = []\n",
    "    for problem in subset_problems:\n",
    "        # Create the same full prompt as batch_quiz_model does\n",
    "        full_prompt =problem \n",
    "        \n",
    "        # Calculate correct answer for the original problem (without prefix/postfix)\n",
    "        correct_answer = calc_correct_answer(problem)\n",
    "        \n",
    "        results.append((full_prompt, correct_answer))\n",
    "    \n",
    "    print(f\"Calculated correct answers for batches {start_batch} to {end_batch-1} ({len(subset_problems)} problems)\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4ba95d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_first_digits(model_answer, correct_answer):\n",
    "    \"\"\"\n",
    "    Extract the first X digits from the model answer, where X is the length of the correct answer.\n",
    "    Removes any repeated question pattern first and handles step-by-step numbering and negative signs.\n",
    "    \n",
    "    Args:\n",
    "        model_answer: The model's response string\n",
    "        correct_answer: The correct answer as a string\n",
    "    \n",
    "    Returns:\n",
    "        str: First X digits from model answer, or empty string if not enough digits found\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Remove question pattern if it exists (operand1{operation}operand2 with no spaces)\n",
    "    # This pattern matches: digits, operation (+,-,*,/), digits, optional =\n",
    "    question_pattern = r'\\d+[\\+\\-\\*/]\\d+\\s*=?\\s*'\n",
    "    cleaned_answer = re.sub(question_pattern, '', model_answer)\n",
    "    \n",
    "    # Remove step numbering patterns (e.g., \"1.\", \"2.\", etc.)\n",
    "    step_pattern = r'\\b\\d+\\.\\s*'\n",
    "    cleaned_answer = re.sub(step_pattern, '', cleaned_answer)\n",
    "    \n",
    "    # Get the length of the correct answer (including minus sign if present)\n",
    "    target_length = len(correct_answer.strip())\n",
    "    \n",
    "    # Extract the first number (with optional minus sign) from the cleaned model answer\n",
    "    # This pattern matches: optional minus sign followed by digits\n",
    "    number_pattern = r'-?\\d+'\n",
    "    numbers = re.findall(number_pattern, cleaned_answer)\n",
    "    \n",
    "    if numbers:\n",
    "        first_number = numbers[0]\n",
    "        # Truncate to target length if needed\n",
    "        if len(first_number) >= target_length:\n",
    "            return first_number[:target_length]\n",
    "        else:\n",
    "            return first_number\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7334aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model_results, correct_answers):\n",
    "    \"\"\"\n",
    "    Calculate accuracy by comparing extracted numbers from model answers with correct answers.\n",
    "    Handles negative numbers and step-by-step responses.\n",
    "    Skipped examples (no valid numbers found) are counted as incorrect in accuracy calculation.\n",
    "    \n",
    "    Args:\n",
    "        model_results: List of tuples (prompt, model_answer) from batch_quiz_model\n",
    "        correct_answers: List of tuples (prompt, correct_answer) from benchmark_correct_answers\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (accuracy, correct_count, total_count, detailed_results, skipped_count)\n",
    "            - accuracy: Accuracy as a float between 0 and 1 (skipped examples count as incorrect)\n",
    "            - correct_count: Number of correct answers\n",
    "            - total_count: Total number of questions (including skipped)\n",
    "            - detailed_results: List of tuples (prompt, correct_answer, model_answer, extracted_number, is_correct)\n",
    "            - skipped_count: Number of prompts skipped due to no valid numbers found\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    if len(model_results) != len(correct_answers):\n",
    "        raise ValueError(f\"Mismatch in result lengths: {len(model_results)} vs {len(correct_answers)}\")\n",
    "    \n",
    "    correct_count = 0\n",
    "    skipped_count = 0\n",
    "    detailed_results = []\n",
    "    \n",
    "    for i, ((model_prompt, model_answer), (correct_prompt, correct_answer)) in enumerate(zip(model_results, correct_answers)):\n",
    "        # Remove question pattern if it exists\n",
    "        question_pattern = r'\\d+[\\+\\-\\*/]\\d+\\s*=?\\s*'\n",
    "        cleaned_answer = re.sub(question_pattern, '', model_answer)\n",
    "        \n",
    "        # Remove step numbering patterns (e.g., \"1.\", \"2.\", etc.)\n",
    "        step_pattern = r'\\b\\d+\\.\\s*'\n",
    "        cleaned_answer = re.sub(step_pattern, '', cleaned_answer)\n",
    "        \n",
    "        # Extract the first number (with optional minus sign) from the cleaned model answer\n",
    "        number_pattern = r'-?\\d+'\n",
    "        numbers = re.findall(number_pattern, cleaned_answer)\n",
    "        \n",
    "        # Handle skipped cases (no numbers found) - count as incorrect\n",
    "        if not numbers:\n",
    "            skipped_count += 1\n",
    "            extracted_number = \"NO_NUMBER_FOUND\"\n",
    "            is_correct = False\n",
    "        else:\n",
    "            # Get the first extracted number\n",
    "            extracted_number = numbers[0]\n",
    "            # Check if extracted number matches the correct answer\n",
    "            is_correct = extracted_number == correct_answer.strip()\n",
    "            if is_correct:\n",
    "                correct_count += 1\n",
    "        \n",
    "        detailed_results.append((\n",
    "            model_prompt,\n",
    "            correct_answer,\n",
    "            model_answer,\n",
    "            extracted_number,\n",
    "            is_correct\n",
    "        ))\n",
    "    \n",
    "    total_count = len(detailed_results)  # Count all results including skipped\n",
    "    accuracy = correct_count / total_count if total_count > 0 else 0.0\n",
    "    \n",
    "    return accuracy, correct_count, total_count, detailed_results, skipped_count\n",
    "\n",
    "def print_accuracy_report(accuracy, correct_count, total_count, detailed_results, skipped_count=0, show_errors_only=False, max_examples=10):\n",
    "    \"\"\"\n",
    "    Print a detailed accuracy report.\n",
    "    \n",
    "    Args:\n",
    "        accuracy: Accuracy as a float\n",
    "        correct_count: Number of correct answers\n",
    "        total_count: Total number of questions\n",
    "        detailed_results: Detailed results from calculate_accuracy\n",
    "        skipped_count: Number of prompts skipped due to insufficient digits\n",
    "        show_errors_only: If True, only show incorrect answers\n",
    "        max_examples: Maximum number of examples to show\n",
    "    \"\"\"\n",
    "    incorrect_count = total_count - correct_count\n",
    "    print(f\"Accuracy: {accuracy:.3f} ({correct_count}/{total_count})\")\n",
    "    print(f\"Correct: {correct_count}\")\n",
    "    print(f\"Incorrect: {incorrect_count} (including {skipped_count} skipped)\")\n",
    "    if skipped_count > 0:\n",
    "        print(f\"  - Actually incorrect: {incorrect_count - skipped_count}\")\n",
    "        print(f\"  - Skipped (no valid numbers): {skipped_count}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    examples_shown = 0\n",
    "    for prompt, correct, model_answer, extracted, is_correct in detailed_results:\n",
    "        if show_errors_only and is_correct:\n",
    "            continue\n",
    "        \n",
    "        if examples_shown >= max_examples:\n",
    "            break\n",
    "        \n",
    "        status = \"✓\" if is_correct else \"✗\"\n",
    "        skipped_indicator = \" (SKIPPED)\" if extracted == \"NO_NUMBER_FOUND\" else \"\"\n",
    "        print(f\"{status} Prompt: {prompt}\")\n",
    "        print(f\"  Correct: {correct}\")\n",
    "        print(f\"  Model output: '{model_answer}'\")\n",
    "        print(f\"  Extracted: '{extracted}'{skipped_indicator}\")\n",
    "        print()\n",
    "        \n",
    "        examples_shown += 1\n",
    "    \n",
    "    if examples_shown < len(detailed_results):\n",
    "        remaining = len(detailed_results) - examples_shown\n",
    "        print(f\"... and {remaining} more examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd513631",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c77954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import psutil\n",
    "import shutil\n",
    "\n",
    "# Optional: Use pynvml if you want more structured GPU info\n",
    "try:\n",
    "    from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n",
    "    pynvml_available = True\n",
    "except ImportError:\n",
    "    pynvml_available = False\n",
    "\n",
    "def show_gpu_usage():\n",
    "    print(\"=== GPU ===\")\n",
    "    if pynvml_available:\n",
    "        nvmlInit()\n",
    "        handle = nvmlDeviceGetHandleByIndex(0)  # GPU 0\n",
    "        meminfo = nvmlDeviceGetMemoryInfo(handle)\n",
    "        print(f\"Total: {meminfo.total / 1024**2:.2f} MB\")\n",
    "        print(f\"Used:  {meminfo.used / 1024**2:.2f} MB\")\n",
    "        print(f\"Free:  {meminfo.free / 1024**2:.2f} MB\")\n",
    "    else:\n",
    "        print(subprocess.getoutput(\"nvidia-smi\"))\n",
    "\n",
    "def show_ram_usage():\n",
    "    print(\"\\n=== RAM ===\")\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"Total: {mem.total / 1024**3:.2f} GB\")\n",
    "    print(f\"Used:  {mem.used / 1024**3:.2f} GB\")\n",
    "    print(f\"Free:  {mem.available / 1024**3:.2f} GB\")\n",
    "\n",
    "def show_disk_usage(path=\"/\"):\n",
    "    print(\"\\n=== Disk ===\")\n",
    "    total, used, free = shutil.disk_usage(path)\n",
    "    print(f\"Total: {total / 1024**3:.2f} GB\")\n",
    "    print(f\"Used:  {used / 1024**3:.2f} GB\")\n",
    "    print(f\"Free:  {free / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4443a6",
   "metadata": {},
   "source": [
    "# Benchmark model performnce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "50235917",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024 * 4  # Set your desired batch size\n",
    "start_batch = 0  # Starting batch index\n",
    "end_batch = 3    # Ending batch index (exclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6357cd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPU ===\n",
      "Sun Aug 10 14:29:33 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  |   00000000:19:00.0 Off |                   On |\n",
      "| N/A   30C    P0             62W /  300W |   12652MiB /  81920MiB |     N/A      Default |\n",
      "|                                         |                        |              Enabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          On  |   00000000:1A:00.0 Off |                   On |\n",
      "| N/A   30C    P0             42W /  300W |     213MiB /  81920MiB |     N/A      Default |\n",
      "|                                         |                        |              Enabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          On  |   00000000:1B:00.0 Off |                   On |\n",
      "| N/A   28C    P0             41W /  300W |     213MiB /  81920MiB |     N/A      Default |\n",
      "|                                         |                        |              Enabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          On  |   00000000:1C:00.0 Off |                   On |\n",
      "| N/A   30C    P0             43W /  300W |     213MiB /  81920MiB |     N/A      Default |\n",
      "|                                         |                        |              Enabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA A100 80GB PCIe          On  |   00000000:B3:00.0 Off |                   On |\n",
      "| N/A   34C    P0             75W /  300W |     562MiB /  81920MiB |     N/A      Default |\n",
      "|                                         |                        |              Enabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA A100 80GB PCIe          On  |   00000000:B4:00.0 Off |                   On |\n",
      "| N/A   30C    P0             43W /  300W |     213MiB /  81920MiB |     N/A      Default |\n",
      "|                                         |                        |              Enabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA A100 80GB PCIe          On  |   00000000:B5:00.0 Off |                   On |\n",
      "| N/A   30C    P0             42W /  300W |     213MiB /  81920MiB |     N/A      Default |\n",
      "|                                         |                        |              Enabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA A100 80GB PCIe          On  |   00000000:B6:00.0 Off |                   On |\n",
      "| N/A   29C    P0             43W /  300W |     213MiB /  81920MiB |     N/A      Default |\n",
      "|                                         |                        |              Enabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                            |\n",
      "+------------------+----------------------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |                     Memory-Usage |        Vol|        Shared         |\n",
      "|      ID  ID  Dev |                       BAR1-Usage | SM     Unc| CE ENC  DEC  OFA  JPG |\n",
      "|                  |                                  |        ECC|                       |\n",
      "|==================+==================================+===========+=======================|\n",
      "|  0    1   0   0  |           12546MiB / 40192MiB    | 42      0 |  3   0    2    0    0 |\n",
      "|                  |                 2MiB / 65535MiB  |           |                       |\n",
      "+------------------+----------------------------------+-----------+-----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0    1    0          3954278      C   /opt/conda/bin/python3.11             12430MiB |\n",
      "|    4    1    0          2915710      C   /usr/bin/python                         340MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "=== RAM ===\n",
      "Total: 1007.53 GB\n",
      "Used:  9.12 GB\n",
      "Free:  991.98 GB\n",
      "\n",
      "=== Disk ===\n",
      "Total: 0.02 GB\n",
      "Used:  0.00 GB\n",
      "Free:  0.02 GB\n"
     ]
    }
   ],
   "source": [
    "show_gpu_usage()\n",
    "show_ram_usage()\n",
    "show_disk_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99236feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton disabled - using PyTorch's default implementations\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Disable Triton since no compiler found in container\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
    "os.environ[\"USE_TRITON\"] = \"0\"\n",
    "print(\"Triton disabled - using PyTorch's default implementations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b0a18ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4096/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8192/10000 prompts\n",
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "Accuracy: 0.269 (2686/10000)\n",
      "Correct: 2686\n",
      "Incorrect: 7314 (including 652 skipped)\n",
      "  - Actually incorrect: 6662\n",
      "  - Skipped (no valid numbers): 652\n",
      "--------------------------------------------------------------------------------\n",
      "✗ Prompt: 8270+1860=\n",
      "  Correct: 10130\n",
      "  Model output: 'Please provide the answer and the steps.\n",
      "\n",
      "**Answer'\n",
      "  Extracted: 'NO_NUMBER_FOUND' (SKIPPED)\n",
      "\n",
      "✗ Prompt: 6734+7265=\n",
      "  Correct: 13999\n",
      "  Model output: '14999\n",
      "\n",
      "14999+'\n",
      "  Extracted: '14999'\n",
      "\n",
      "✗ Prompt: 1466+5426=\n",
      "  Correct: 6892\n",
      "  Model output: 'Here's how to solve it:\n",
      "\n",
      "**1'\n",
      "  Extracted: '1'\n",
      "\n",
      "✗ Prompt: 6578+9322=\n",
      "  Correct: 15900\n",
      "  Model output: '16890\n",
      "\n",
      "16890-'\n",
      "  Extracted: '16890'\n",
      "\n",
      "✗ Prompt: 7949+3433=\n",
      "  Correct: 11382\n",
      "  Model output: 'Here's how to solve it:\n",
      "\n",
      "**1'\n",
      "  Extracted: '1'\n",
      "\n",
      "✗ Prompt: 7420+2184=\n",
      "  Correct: 9604\n",
      "  Model output: '**Answer:** \n",
      "\n",
      "**Answer:**'\n",
      "  Extracted: 'NO_NUMBER_FOUND' (SKIPPED)\n",
      "\n",
      "✗ Prompt: 7396+9666=\n",
      "  Correct: 17062\n",
      "  Model output: 'Here's how to solve it:\n",
      "\n",
      "**1'\n",
      "  Extracted: '1'\n",
      "\n",
      "✗ Prompt: 3047+3747=\n",
      "  Correct: 6794\n",
      "  Model output: '7494\n",
      "\n",
      "7494+37'\n",
      "  Extracted: '7494'\n",
      "\n",
      "✗ Prompt: 1189+3734=\n",
      "  Correct: 4923\n",
      "  Model output: '5923\n",
      "\n",
      "1189+37'\n",
      "  Extracted: '5923'\n",
      "\n",
      "✗ Prompt: 4005+5658=\n",
      "  Correct: 9663\n",
      "  Model output: '10663\n",
      "\n",
      "10663+'\n",
      "  Extracted: '10663'\n",
      "\n",
      "... and 9990 more examples\n"
     ]
    }
   ],
   "source": [
    "prefix = \"\"\n",
    "postfix = \"\"\n",
    "results = batch_quiz_model(\"./data/random_addition.txt\", max_new_tokens=12 ,batch_size=batch_size, start_batch=start_batch, end_batch=end_batch, prefix=prefix, postfix=postfix)\n",
    "\n",
    "correct_answers = benchmark_correct_answers(\"./data/random_addition.txt\", batch_size=batch_size, start_batch=start_batch, end_batch=end_batch)\n",
    "\n",
    "# Calculate accuracy with skipping\n",
    "accuracy, correct_count, total_count, detailed_results, skipped_count = calculate_accuracy(results, correct_answers)\n",
    "\n",
    "# Print report including skipped count\n",
    "print_accuracy_report(accuracy, correct_count, total_count, detailed_results, skipped_count, show_errors_only=True, max_examples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b3f301c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def run_comprehensive_evaluation(batch_size=1024*4, start_batch=0, end_batch=3, \n",
    "                                max_new_tokens=12, output_dir=\"./answers\"):\n",
    "    \"\"\"\n",
    "    Run all datasets with all prefix/postfix combinations and save results.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Batch size for processing\n",
    "        start_batch: Starting batch index\n",
    "        end_batch: Ending batch index\n",
    "        max_new_tokens: Maximum new tokens to generate\n",
    "        output_dir: Directory to save results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Define datasets\n",
    "    datasets = [\n",
    "        \"./data/addition.txt\",\n",
    "        \"./data/random_addition.txt\", \n",
    "        \"./data/subtraction.txt\",\n",
    "        \"./data/random_subtraction.txt\"\n",
    "    ]\n",
    "    \n",
    "    # Define prefix/postfix combinations\n",
    "    prompt_combinations = [\n",
    "        {\"name\": \"no_prompt\", \"prefix\": \"\", \"postfix\": \"\"},\n",
    "        {\"name\": \"answer_directly\", \"prefix\": \"Answer directly: \", \"postfix\": \" \"},\n",
    "        {\"name\": \"answer_suffix\", \"prefix\": \"\", \"postfix\": \" Answer: \"},\n",
    "        {\"name\": \"space_suffix\", \"prefix\": \"\", \"postfix\": \" \"},\n",
    "        {\"name\": \"final_answer\", \"prefix\": \"\", \"postfix\": \" final answer: \"},\n",
    "        {\"name\": \"calc_prefix\", \"prefix\": \"calc: \", \"postfix\": \"\"}\n",
    "    ]\n",
    "    \n",
    "    # Store all metrics\n",
    "    all_metrics = {}\n",
    "    \n",
    "    # Get timestamp for this run\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    print(f\"Starting comprehensive evaluation at {timestamp}\")\n",
    "    print(f\"Will process {len(datasets)} datasets with {len(prompt_combinations)} prompt combinations\")\n",
    "    print(f\"Total combinations: {len(datasets) * len(prompt_combinations)}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    combination_count = 0\n",
    "    total_combinations = len(datasets) * len(prompt_combinations)\n",
    "    \n",
    "    for dataset_path in datasets:\n",
    "        dataset_name = os.path.basename(dataset_path).replace('.txt', '')\n",
    "        print(f\"\\nProcessing dataset: {dataset_name}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if dataset_name not in all_metrics:\n",
    "            all_metrics[dataset_name] = {}\n",
    "        \n",
    "        for prompt_combo in prompt_combinations:\n",
    "            combination_count += 1\n",
    "            combo_name = prompt_combo[\"name\"]\n",
    "            prefix = prompt_combo[\"prefix\"]\n",
    "            postfix = prompt_combo[\"postfix\"]\n",
    "            \n",
    "            print(f\"[{combination_count}/{total_combinations}] Running {dataset_name} with {combo_name}\")\n",
    "            print(f\"  Prefix: '{prefix}' | Postfix: '{postfix}'\")\n",
    "            \n",
    "            try:\n",
    "                # Run the model\n",
    "                results = batch_quiz_model(\n",
    "                    dataset_path, \n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    batch_size=batch_size, \n",
    "                    start_batch=start_batch, \n",
    "                    end_batch=end_batch, \n",
    "                    prefix=prefix, \n",
    "                    postfix=postfix\n",
    "                )\n",
    "                \n",
    "                # Get correct answers\n",
    "                correct_answers = benchmark_correct_answers(\n",
    "                    dataset_path, \n",
    "                    batch_size=batch_size, \n",
    "                    start_batch=start_batch, \n",
    "                    end_batch=end_batch\n",
    "                )\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                accuracy, correct_count, total_count, detailed_results, skipped_count = calculate_accuracy(\n",
    "                    results, correct_answers\n",
    "                )\n",
    "                \n",
    "                # Save detailed results to text file\n",
    "                answers_filename = f\"{dataset_name}_{combo_name}_{timestamp}.txt\"\n",
    "                answers_filepath = os.path.join(output_dir, answers_filename)\n",
    "                \n",
    "                with open(answers_filepath, 'w') as f:\n",
    "                    f.write(f\"Dataset: {dataset_name}\\n\")\n",
    "                    f.write(f\"Prompt: prefix='{prefix}' postfix='{postfix}'\\n\")\n",
    "                    f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "                    f.write(f\"Batch range: {start_batch} to {end_batch-1}\\n\")\n",
    "                    f.write(f\"Max new tokens: {max_new_tokens}\\n\")\n",
    "                    f.write(f\"Accuracy: {accuracy:.3f} ({correct_count}/{total_count})\\n\")\n",
    "                    f.write(f\"Skipped: {skipped_count}\\n\")\n",
    "                    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "                    \n",
    "                    for prompt, correct, model_answer, extracted, is_correct in detailed_results:\n",
    "                        status = \"✓\" if is_correct else \"✗\"\n",
    "                        skipped_indicator = \" (SKIPPED)\" if extracted == \"NO_NUMBER_FOUND\" else \"\"\n",
    "                        f.write(f\"{status} Problem: {prompt.replace(prefix, '').replace(postfix, '')}\\n\")\n",
    "                        f.write(f\"  Correct: {correct}\\n\")\n",
    "                        f.write(f\"  Model output: '{model_answer}'\\n\")\n",
    "                        f.write(f\"  Extracted: '{extracted}'{skipped_indicator}\\n\")\n",
    "                        f.write(\"\\n\")\n",
    "                \n",
    "                # Store metrics\n",
    "                all_metrics[dataset_name][combo_name] = {\n",
    "                    \"accuracy\": accuracy,\n",
    "                    \"correct_count\": correct_count,\n",
    "                    \"total_count\": total_count,\n",
    "                    \"incorrect_count\": total_count - correct_count,\n",
    "                    \"skipped_count\": skipped_count,\n",
    "                    \"prefix\": prefix,\n",
    "                    \"postfix\": postfix,\n",
    "                    \"max_new_tokens\": max_new_tokens,\n",
    "                    \"batch_range\": f\"{start_batch}-{end_batch-1}\",\n",
    "                    \"answers_file\": answers_filename\n",
    "                }\n",
    "                \n",
    "                print(f\"  Results: {accuracy:.3f} accuracy ({correct_count}/{total_count}), {skipped_count} skipped\")\n",
    "                print(f\"  Saved to: {answers_filename}\")\n",
    "                \n",
    "                # Clean up memory\n",
    "                del results, correct_answers, detailed_results\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR: {str(e)}\")\n",
    "                all_metrics[dataset_name][combo_name] = {\n",
    "                    \"error\": str(e),\n",
    "                    \"prefix\": prefix,\n",
    "                    \"postfix\": postfix\n",
    "                }\n",
    "    \n",
    "    # Save all metrics to JSON\n",
    "    metrics_filename = f\"evaluation_metrics_{timestamp}.json\"\n",
    "    metrics_filepath = os.path.join(output_dir, metrics_filename)\n",
    "    \n",
    "    # Add summary statistics\n",
    "    summary_data = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"evaluation_settings\": {\n",
    "            \"batch_size\": batch_size,\n",
    "            \"start_batch\": start_batch,\n",
    "            \"end_batch\": end_batch,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"batch_range_description\": f\"batches {start_batch} to {end_batch-1}\"\n",
    "        },\n",
    "        \"datasets_processed\": len(datasets),\n",
    "        \"prompt_combinations_processed\": len(prompt_combinations),\n",
    "        \"total_combinations\": total_combinations,\n",
    "        \"metrics\": all_metrics\n",
    "    }\n",
    "    \n",
    "    with open(metrics_filepath, 'w') as f:\n",
    "        json.dump(summary_data, f, indent=2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EVALUATION COMPLETE!\")\n",
    "    print(f\"Processed {total_combinations} combinations\")\n",
    "    print(f\"Metrics saved to: {metrics_filename}\")\n",
    "    print(f\"Individual results saved to: {output_dir}/\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\nSUMMARY TABLE:\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{'Dataset':<20} {'Prompt':<15} {'Accuracy':<10} {'Correct':<8} {'Total':<8} {'Skipped':<8}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for dataset_name in all_metrics:\n",
    "        for combo_name, metrics in all_metrics[dataset_name].items():\n",
    "            if 'accuracy' in metrics:  # Skip error cases\n",
    "                print(f\"{dataset_name:<20} {combo_name:<15} {metrics['accuracy']:<10.3f} \"\n",
    "                      f\"{metrics['correct_count']:<8} {metrics['total_count']:<8} {metrics['skipped_count']:<8}\")\n",
    "    \n",
    "    return all_metrics, metrics_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9a1dcec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive evaluation at 20250810_194802\n",
      "Will process 4 datasets with 6 prompt combinations\n",
      "Total combinations: 24\n",
      "================================================================================\n",
      "\n",
      "Processing dataset: addition\n",
      "----------------------------------------\n",
      "[1/24] Running addition with no_prompt\n",
      "  Prefix: '' | Postfix: ''\n",
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4096/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8192/10000 prompts\n",
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "  Results: 0.173 accuracy (1734/10000), 98 skipped\n",
      "  Saved to: addition_no_prompt_20250810_194802.txt\n",
      "[2/24] Running addition with answer_directly\n",
      "  Prefix: 'Answer directly: ' | Postfix: ' '\n",
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4096/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8192/10000 prompts\n",
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "  Results: 0.963 accuracy (9625/10000), 344 skipped\n",
      "  Saved to: addition_answer_directly_20250810_194802.txt\n",
      "[3/24] Running addition with answer_suffix\n",
      "  Prefix: '' | Postfix: ' Answer: '\n",
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4096/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8192/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "  Results: 0.067 accuracy (672/10000), 291 skipped\n",
      "  Saved to: addition_answer_suffix_20250810_194802.txt\n",
      "[4/24] Running addition with space_suffix\n",
      "  Prefix: '' | Postfix: ' '\n",
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4096/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8192/10000 prompts\n",
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "  Results: 0.295 accuracy (2952/10000), 1686 skipped\n",
      "  Saved to: addition_space_suffix_20250810_194802.txt\n",
      "[5/24] Running addition with final_answer\n",
      "  Prefix: '' | Postfix: ' final answer: '\n",
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4096/10000 prompts\n",
      "Processed 8192/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "  Results: 0.497 accuracy (4966/10000), 820 skipped\n",
      "  Saved to: addition_final_answer_20250810_194802.txt\n",
      "[6/24] Running addition with calc_prefix\n",
      "  Prefix: 'calc: ' | Postfix: ''\n",
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4096/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8192/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "  Results: 0.138 accuracy (1382/10000), 59 skipped\n",
      "  Saved to: addition_calc_prefix_20250810_194802.txt\n",
      "\n",
      "Processing dataset: random_addition\n",
      "----------------------------------------\n",
      "[7/24] Running random_addition with no_prompt\n",
      "  Prefix: '' | Postfix: ''\n",
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4096/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8192/10000 prompts\n",
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "  Results: 0.272 accuracy (2720/10000), 339 skipped\n",
      "  Saved to: random_addition_no_prompt_20250810_194802.txt\n",
      "[8/24] Running random_addition with answer_directly\n",
      "  Prefix: 'Answer directly: ' | Postfix: ' '\n",
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4096/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8192/10000 prompts\n",
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "  Results: 0.574 accuracy (5743/10000), 3482 skipped\n",
      "  Saved to: random_addition_answer_directly_20250810_194802.txt\n",
      "[9/24] Running random_addition with answer_suffix\n",
      "  Prefix: '' | Postfix: ' Answer: '\n",
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4096/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8192/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "  Results: 0.212 accuracy (2119/10000), 370 skipped\n",
      "  Saved to: random_addition_answer_suffix_20250810_194802.txt\n",
      "[10/24] Running random_addition with space_suffix\n",
      "  Prefix: '' | Postfix: ' '\n",
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4096/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8192/10000 prompts\n",
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "  Results: 0.314 accuracy (3144/10000), 1219 skipped\n",
      "  Saved to: random_addition_space_suffix_20250810_194802.txt\n",
      "[11/24] Running random_addition with final_answer\n",
      "  Prefix: '' | Postfix: ' final answer: '\n",
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4096/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8192/10000 prompts\n",
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "  Results: 0.331 accuracy (3311/10000), 1731 skipped\n",
      "  Saved to: random_addition_final_answer_20250810_194802.txt\n",
      "[12/24] Running random_addition with calc_prefix\n",
      "  Prefix: 'calc: ' | Postfix: ''\n",
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4096/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8192/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "  Results: 0.316 accuracy (3163/10000), 119 skipped\n",
      "  Saved to: random_addition_calc_prefix_20250810_194802.txt\n",
      "\n",
      "Processing dataset: subtraction\n",
      "----------------------------------------\n",
      "[13/24] Running subtraction with no_prompt\n",
      "  Prefix: '' | Postfix: ''\n",
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4096/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8192/10000 prompts\n",
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "  Results: 0.263 accuracy (2632/10000), 382 skipped\n",
      "  Saved to: subtraction_no_prompt_20250810_194802.txt\n",
      "[14/24] Running subtraction with answer_directly\n",
      "  Prefix: 'Answer directly: ' | Postfix: ' '\n",
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4096/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8192/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "  Results: 0.907 accuracy (9072/10000), 310 skipped\n",
      "  Saved to: subtraction_answer_directly_20250810_194802.txt\n",
      "[15/24] Running subtraction with answer_suffix\n",
      "  Prefix: '' | Postfix: ' Answer: '\n",
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4096/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8192/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "  Results: 0.244 accuracy (2438/10000), 1037 skipped\n",
      "  Saved to: subtraction_answer_suffix_20250810_194802.txt\n",
      "[16/24] Running subtraction with space_suffix\n",
      "  Prefix: '' | Postfix: ' '\n",
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4096/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8192/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "  Results: 0.220 accuracy (2202/10000), 3580 skipped\n",
      "  Saved to: subtraction_space_suffix_20250810_194802.txt\n",
      "[17/24] Running subtraction with final_answer\n",
      "  Prefix: '' | Postfix: ' final answer: '\n",
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n",
      "Processed 4096/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8192/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "  Results: 0.485 accuracy (4851/10000), 2172 skipped\n",
      "  Saved to: subtraction_final_answer_20250810_194802.txt\n",
      "[18/24] Running subtraction with calc_prefix\n",
      "  Prefix: 'calc: ' | Postfix: ''\n",
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4096/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8192/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "  Results: 0.040 accuracy (401/10000), 127 skipped\n",
      "  Saved to: subtraction_calc_prefix_20250810_194802.txt\n",
      "\n",
      "Processing dataset: random_subtraction\n",
      "----------------------------------------\n",
      "[19/24] Running random_subtraction with no_prompt\n",
      "  Prefix: '' | Postfix: ''\n",
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4096/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8192/10000 prompts\n",
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "  Results: 0.243 accuracy (2427/10000), 916 skipped\n",
      "  Saved to: random_subtraction_no_prompt_20250810_194802.txt\n",
      "[20/24] Running random_subtraction with answer_directly\n",
      "  Prefix: 'Answer directly: ' | Postfix: ' '\n",
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4096/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8192/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "  Results: 0.591 accuracy (5907/10000), 2655 skipped\n",
      "  Saved to: random_subtraction_answer_directly_20250810_194802.txt\n",
      "[21/24] Running random_subtraction with answer_suffix\n",
      "  Prefix: '' | Postfix: ' Answer: '\n",
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4096/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8192/10000 prompts\n",
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "  Results: 0.229 accuracy (2286/10000), 1660 skipped\n",
      "  Saved to: random_subtraction_answer_suffix_20250810_194802.txt\n",
      "[22/24] Running random_subtraction with space_suffix\n",
      "  Prefix: '' | Postfix: ' '\n",
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4096/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8192/10000 prompts\n",
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "  Results: 0.161 accuracy (1610/10000), 3832 skipped\n",
      "  Saved to: random_subtraction_space_suffix_20250810_194802.txt\n",
      "[23/24] Running random_subtraction with final_answer\n",
      "  Prefix: '' | Postfix: ' final answer: '\n",
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4096/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8192/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "  Results: 0.267 accuracy (2665/10000), 2258 skipped\n",
      "  Saved to: random_subtraction_final_answer_20250810_194802.txt\n",
      "[24/24] Running random_subtraction with calc_prefix\n",
      "  Prefix: 'calc: ' | Postfix: ''\n",
      "Processing batches 0 to 2 (10000 problems) in batches of 4096...\n",
      "Total dataset size: 10000 problems (3 total batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4096/10000 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8192/10000 prompts\n",
      "Processed 10000/10000 prompts\n",
      "Calculated correct answers for batches 0 to 2 (10000 problems)\n",
      "  Results: 0.054 accuracy (545/10000), 80 skipped\n",
      "  Saved to: random_subtraction_calc_prefix_20250810_194802.txt\n",
      "\n",
      "================================================================================\n",
      "EVALUATION COMPLETE!\n",
      "Processed 24 combinations\n",
      "Metrics saved to: evaluation_metrics_20250810_194802.json\n",
      "Individual results saved to: ./answers/\n",
      "================================================================================\n",
      "\n",
      "SUMMARY TABLE:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Dataset              Prompt          Accuracy   Correct  Total    Skipped \n",
      "----------------------------------------------------------------------------------------------------\n",
      "addition             no_prompt       0.173      1734     10000    98      \n",
      "addition             answer_directly 0.963      9625     10000    344     \n",
      "addition             answer_suffix   0.067      672      10000    291     \n",
      "addition             space_suffix    0.295      2952     10000    1686    \n",
      "addition             final_answer    0.497      4966     10000    820     \n",
      "addition             calc_prefix     0.138      1382     10000    59      \n",
      "random_addition      no_prompt       0.272      2720     10000    339     \n",
      "random_addition      answer_directly 0.574      5743     10000    3482    \n",
      "random_addition      answer_suffix   0.212      2119     10000    370     \n",
      "random_addition      space_suffix    0.314      3144     10000    1219    \n",
      "random_addition      final_answer    0.331      3311     10000    1731    \n",
      "random_addition      calc_prefix     0.316      3163     10000    119     \n",
      "subtraction          no_prompt       0.263      2632     10000    382     \n",
      "subtraction          answer_directly 0.907      9072     10000    310     \n",
      "subtraction          answer_suffix   0.244      2438     10000    1037    \n",
      "subtraction          space_suffix    0.220      2202     10000    3580    \n",
      "subtraction          final_answer    0.485      4851     10000    2172    \n",
      "subtraction          calc_prefix     0.040      401      10000    127     \n",
      "random_subtraction   no_prompt       0.243      2427     10000    916     \n",
      "random_subtraction   answer_directly 0.591      5907     10000    2655    \n",
      "random_subtraction   answer_suffix   0.229      2286     10000    1660    \n",
      "random_subtraction   space_suffix    0.161      1610     10000    3832    \n",
      "random_subtraction   final_answer    0.267      2665     10000    2258    \n",
      "random_subtraction   calc_prefix     0.054      545      10000    80      \n"
     ]
    }
   ],
   "source": [
    "# Run the comprehensive evaluation\n",
    "batch_size = 1024 * 4  # Your current batch size\n",
    "start_batch = 0  # Your current start batch\n",
    "end_batch = 3    # Your current end batch\n",
    "\n",
    "# Run all combinations\n",
    "all_metrics, metrics_file = run_comprehensive_evaluation(\n",
    "    batch_size=batch_size,\n",
    "    start_batch=start_batch, \n",
    "    end_batch=end_batch,\n",
    "    max_new_tokens=25,  # Using 25 to handle longer subtraction answers\n",
    "    output_dir=\"./answers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "94b6f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(metrics_file_path):\n",
    "    \"\"\"\n",
    "    Load and analyze the evaluation results.\n",
    "    \n",
    "    Args:\n",
    "        metrics_file_path: Path to the metrics JSON file\n",
    "    \"\"\"\n",
    "    with open(metrics_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(\"DETAILED ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Evaluation timestamp: {data['timestamp']}\")\n",
    "    print(f\"Settings: {data['evaluation_settings']}\")\n",
    "    print()\n",
    "    \n",
    "    # Find best and worst performing combinations\n",
    "    all_results = []\n",
    "    for dataset_name, dataset_metrics in data['metrics'].items():\n",
    "        for combo_name, metrics in dataset_metrics.items():\n",
    "            if 'accuracy' in metrics:  # Skip error cases\n",
    "                all_results.append({\n",
    "                    'dataset': dataset_name,\n",
    "                    'prompt': combo_name,\n",
    "                    'accuracy': metrics['accuracy'],\n",
    "                    'correct': metrics['correct_count'],\n",
    "                    'total': metrics['total_count'],\n",
    "                    'skipped': metrics['skipped_count'],\n",
    "                    'prefix': metrics['prefix'],\n",
    "                    'postfix': metrics['postfix']\n",
    "                })\n",
    "    \n",
    "    # Sort by accuracy\n",
    "    all_results.sort(key=lambda x: x['accuracy'], reverse=True)\n",
    "    \n",
    "    print(\"TOP 5 PERFORMING COMBINATIONS:\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, result in enumerate(all_results[:5]):\n",
    "        print(f\"{i+1}. {result['dataset']} + {result['prompt']}: {result['accuracy']:.3f}\")\n",
    "        print(f\"   Prefix: '{result['prefix']}' | Postfix: '{result['postfix']}'\")\n",
    "        print(f\"   Correct: {result['correct']}/{result['total']}, Skipped: {result['skipped']}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"BOTTOM 5 PERFORMING COMBINATIONS:\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, result in enumerate(all_results[-5:]):\n",
    "        print(f\"{i+1}. {result['dataset']} + {result['prompt']}: {result['accuracy']:.3f}\")\n",
    "        print(f\"   Prefix: '{result['prefix']}' | Postfix: '{result['postfix']}'\")\n",
    "        print(f\"   Correct: {result['correct']}/{result['total']}, Skipped: {result['skipped']}\")\n",
    "        print()\n",
    "    \n",
    "    # Dataset-wise performance\n",
    "    print(\"DATASET-WISE AVERAGE PERFORMANCE:\")\n",
    "    print(\"-\" * 50)\n",
    "    dataset_stats = {}\n",
    "    for result in all_results:\n",
    "        if result['dataset'] not in dataset_stats:\n",
    "            dataset_stats[result['dataset']] = []\n",
    "        dataset_stats[result['dataset']].append(result['accuracy'])\n",
    "    \n",
    "    for dataset, accuracies in dataset_stats.items():\n",
    "        avg_acc = sum(accuracies) / len(accuracies)\n",
    "        print(f\"{dataset}: {avg_acc:.3f} (min: {min(accuracies):.3f}, max: {max(accuracies):.3f})\")\n",
    "    \n",
    "    # Prompt-wise performance\n",
    "    print(\"\\nPROMPT-WISE AVERAGE PERFORMANCE:\")\n",
    "    print(\"-\" * 50)\n",
    "    prompt_stats = {}\n",
    "    for result in all_results:\n",
    "        if result['prompt'] not in prompt_stats:\n",
    "            prompt_stats[result['prompt']] = []\n",
    "        prompt_stats[result['prompt']].append(result['accuracy'])\n",
    "    \n",
    "    for prompt, accuracies in prompt_stats.items():\n",
    "        avg_acc = sum(accuracies) / len(accuracies)\n",
    "        print(f\"{prompt}: {avg_acc:.3f} (min: {min(accuracies):.3f}, max: {max(accuracies):.3f})\")\n",
    "    \n",
    "    return data, all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "06ad878c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETAILED ANALYSIS\n",
      "================================================================================\n",
      "Evaluation timestamp: 20250810_194802\n",
      "Settings: {'batch_size': 4096, 'start_batch': 0, 'end_batch': 3, 'max_new_tokens': 25, 'batch_range_description': 'batches 0 to 2'}\n",
      "\n",
      "TOP 5 PERFORMING COMBINATIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "1. addition + answer_directly: 0.963\n",
      "   Prefix: 'Answer directly: ' | Postfix: ' '\n",
      "   Correct: 9625/10000, Skipped: 344\n",
      "\n",
      "2. subtraction + answer_directly: 0.907\n",
      "   Prefix: 'Answer directly: ' | Postfix: ' '\n",
      "   Correct: 9072/10000, Skipped: 310\n",
      "\n",
      "3. random_subtraction + answer_directly: 0.591\n",
      "   Prefix: 'Answer directly: ' | Postfix: ' '\n",
      "   Correct: 5907/10000, Skipped: 2655\n",
      "\n",
      "4. random_addition + answer_directly: 0.574\n",
      "   Prefix: 'Answer directly: ' | Postfix: ' '\n",
      "   Correct: 5743/10000, Skipped: 3482\n",
      "\n",
      "5. addition + final_answer: 0.497\n",
      "   Prefix: '' | Postfix: ' final answer: '\n",
      "   Correct: 4966/10000, Skipped: 820\n",
      "\n",
      "BOTTOM 5 PERFORMING COMBINATIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "1. random_subtraction + space_suffix: 0.161\n",
      "   Prefix: '' | Postfix: ' '\n",
      "   Correct: 1610/10000, Skipped: 3832\n",
      "\n",
      "2. addition + calc_prefix: 0.138\n",
      "   Prefix: 'calc: ' | Postfix: ''\n",
      "   Correct: 1382/10000, Skipped: 59\n",
      "\n",
      "3. addition + answer_suffix: 0.067\n",
      "   Prefix: '' | Postfix: ' Answer: '\n",
      "   Correct: 672/10000, Skipped: 291\n",
      "\n",
      "4. random_subtraction + calc_prefix: 0.054\n",
      "   Prefix: 'calc: ' | Postfix: ''\n",
      "   Correct: 545/10000, Skipped: 80\n",
      "\n",
      "5. subtraction + calc_prefix: 0.040\n",
      "   Prefix: 'calc: ' | Postfix: ''\n",
      "   Correct: 401/10000, Skipped: 127\n",
      "\n",
      "DATASET-WISE AVERAGE PERFORMANCE:\n",
      "--------------------------------------------------\n",
      "addition: 0.356 (min: 0.067, max: 0.963)\n",
      "subtraction: 0.360 (min: 0.040, max: 0.907)\n",
      "random_subtraction: 0.257 (min: 0.054, max: 0.591)\n",
      "random_addition: 0.337 (min: 0.212, max: 0.574)\n",
      "\n",
      "PROMPT-WISE AVERAGE PERFORMANCE:\n",
      "--------------------------------------------------\n",
      "answer_directly: 0.759 (min: 0.574, max: 0.963)\n",
      "final_answer: 0.395 (min: 0.267, max: 0.497)\n",
      "calc_prefix: 0.137 (min: 0.040, max: 0.316)\n",
      "space_suffix: 0.248 (min: 0.161, max: 0.314)\n",
      "no_prompt: 0.238 (min: 0.173, max: 0.272)\n",
      "answer_suffix: 0.188 (min: 0.067, max: 0.244)\n"
     ]
    }
   ],
   "source": [
    "# Analyze the results\n",
    "data, all_results = analyze_results(metrics_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca06f206",
   "metadata": {},
   "source": [
    "# Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b1020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Modification Hook\n",
    "class AblationHook:\n",
    "    def __init__(self, layer_idx, lambda_scale,):\n",
    "        \"\"\"\n",
    "        Initialize the ablation hook.\n",
    "        \n",
    "        Args:\n",
    "            sae: Sparse Autoencoder instance\n",
    "            lambda_scale: Scaling factor for the single latent reconstruction\n",
    "        \"\"\"\n",
    "        # Initialize SAE for layer 12\n",
    "        sae_id = f\"layer_{layer_idx}/width_16k/canonical\"\n",
    "        sae, _, _ = SAE.from_pretrained(\n",
    "            release=\"gemma-scope-2b-pt-res-canonical\",\n",
    "            sae_id=sae_id\n",
    "        )\n",
    "\n",
    "        model_dtype = next(model.parameters()).dtype\n",
    "        sae = sae.to(device=device, dtype=model_dtype)\n",
    "        sae.eval()\n",
    "\n",
    "        self.sae = sae\n",
    "        self.lambda_scale = lambda_scale\n",
    "        self.sae_hook_handles = []\n",
    "    \n",
    "\n",
    "    # Single Latent Reconstruction Hook\n",
    "    def single_latent_hook(self, module, input, output, latent_idx):\n",
    "        \"\"\"\n",
    "        Forward hook to zero all latent activations except for the specified index.\n",
    "        \n",
    "        Args:\n",
    "            module: The HookPoint module (hook_sae_acts_post)\n",
    "            input: Input to the hook (not used here)\n",
    "            output: feature_acts tensor of shape [..., d_sae]\n",
    "            latent_idx: Index of the latent to keep non-zero\n",
    "        \n",
    "        Returns:\n",
    "            Modified feature_acts with only latent_idx non-zero\n",
    "        \"\"\"\n",
    "        modified_acts = torch.zeros_like(output)\n",
    "        modified_acts[..., latent_idx] = output[..., latent_idx]\n",
    "        # validate that the modified acts at the latent index are the same as the no ablation case. \n",
    "        # print(f\"Modified acts at latent index {latent_idx}: {modified_acts[..., latent_idx]}\")\n",
    "\n",
    "        return modified_acts # TODO: DO I need to use an activation function here?\n",
    "    \n",
    "\n",
    "    # Register the activation modification hook\n",
    "    def register_hook(self, latent_idx):\n",
    "        # Register hook to capture single latent reconstruction\n",
    "        self.sae_hook_handles.append(self.sae.hook_sae_acts_post.register_forward_hook(\n",
    "            lambda m, i, o: self.single_latent_hook(m, i, o, latent_idx=latent_idx)\n",
    "        ))\n",
    "\n",
    "    def unregister_hook(self):\n",
    "        if hasattr(self, 'sae_hook_handles'):\n",
    "            for hook_handle in self.sae_hook_handles:\n",
    "                hook_handle.remove()\n",
    "        self.sae_hook_handles = []\n",
    "\n",
    "    # def run_hooked_sae(self, hidden_states):\n",
    "    def activation_modification_hook(self, module, input, output):\n",
    "        hidden_states = output[0]\n",
    "        \n",
    "        # Compute single latent reconstruction on current hidden_states\n",
    "        with torch.no_grad():\n",
    "                reconstruction_w_hooks = self.sae(hidden_states)\n",
    "        \n",
    "        modified_hidden_states = hidden_states - self.lambda_scale * reconstruction_w_hooks\n",
    "        \n",
    "        return modified_hidden_states # TODO: check that this has the correct shape, value, and dtype. \n",
    "\n",
    "    def run_encoding_decoding(self, hidden_states):\n",
    "        \"\"\"\n",
    "        Run encoding and Calculate the reconstruction loss for the given hidden states.\n",
    "        \n",
    "        Args:\n",
    "            hidden_states: The hidden states to calculate the loss for\n",
    "            \n",
    "        Returns:\n",
    "            The reconstruction loss\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            latents = self.sae.encode(hidden_states)\n",
    "            # Compute single latent reconstruction on current hidden_states\n",
    "            reconstruction = self.sae(hidden_states)\n",
    "            # Calculate the mean squared error loss\n",
    "            loss = F.mse_loss(reconstruction, hidden_states)\n",
    "        return reconstruction, latents, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2efad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment settings\n",
    "import os\n",
    "\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
    "os.environ[\"USE_TRITON\"] = \"0\"\n",
    "layer_idx = 12  # Layer to modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ba471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"5+7= \"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(device)\n",
    "print(f\"Input tokens: {inputs}\")\n",
    "\n",
    "# Get original hidden states\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs, output_hidden_states=True)\n",
    "# print(outputs.shape)\n",
    "print(len(outputs.hidden_states))\n",
    "# original_hidden_states = outputs.hidden_states[layer_idx, -1, :]\n",
    "# print(f\"Original hidden states shape: {original_hidden_states.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecca2d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(outputs.hidden_states[layer_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70157956",
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_hook = AblationHook(layer_idx=12, lambda_scale = 1.0,)\n",
    "reconstruction, latents, loss = ablation_hook.run_encoding_decoding(original_hidden_states)\n",
    "ablation_hook.register_hook(latent_idx=11301)\n",
    "ablation_reconstruction, ablation_latents, ablation_loss = ablation_hook.run_encoding_decoding(original_hidden_states)\n",
    "\n",
    "print(reconstruction.shape)\n",
    "print(f\"Reconstruction loss: {loss.item()}\")\n",
    "print(f\"Ablation reconstruction shape: {ablation_reconstruction.shape}\")\n",
    "print(f\"Ablation reconstruction loss: {ablation_loss.item()}\")\n",
    "print(f\"Latents shape: {latents.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d0c0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents[0,:, 11301] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295d4209",
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_latents[0,:, 11301]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584bb779",
   "metadata": {},
   "source": [
    "## Quick check: are latents zero for all tokens except the first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9383dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print current dir\n",
    "import os\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad40477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "latents_npz_path = r\"../../latents/addition/layer_12.npz\"\n",
    "latents_npz = sparse.load_npz(latents_npz_path)\n",
    "print(latents_npz.shape)  # List all arrays in the npz file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3ebd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latents_npz[:,11301].toarray())  # Print the latents for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a89644c",
   "metadata": {},
   "source": [
    "# Misc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766e0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_generation(prompt, max_new_tokens=12):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(device)\n",
    "    print(f\"Input tokens: {inputs}\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=max_new_tokens,              # Limit tokens for just the number\n",
    "            do_sample=False,                # Deterministic output\n",
    "            repetition_penalty=1.0,         # Avoid penalizing repeated digits\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            num_beams=1,                  # Single beam for simplicity\n",
    "            # # Stop at common continuation tokens\n",
    "            # bad_words_ids=[[tokenizer.encode(\" The\")[0]], \n",
    "            #               [tokenizer.encode(\" So\")[0]],\n",
    "            #               [tokenizer.encode(\"\\n\")[0]],\n",
    "            #               [tokenizer.encode(\"Here\")[0]],\n",
    "            #               ]\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
    "    # Extract just the number\n",
    "    return response.strip()  # Assuming the model outputs just the number\n",
    "    # number_match = re.search(r'^\\s*(\\d+)', response.strip())\n",
    "    # return number_match.group(1) if number_match else response.strip()\n",
    "    \n",
    "def quiz_model(dataset: str):\n",
    "    \"\"\"\n",
    "    Quiz the model on a dataset of arithmetic problems.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    with open(dataset, 'r') as f:\n",
    "        problems = f.readlines()\n",
    "    \n",
    "    results = []\n",
    "    for problem in problems:\n",
    "        problem = problem.strip()\n",
    "        problem = \"You are a calculator. Answer immediately: \" + problem\n",
    "        if problem:\n",
    "            answer = quick_generation(problem, max_new_tokens=10)\n",
    "            results.append((problem, answer))\n",
    "            print(answer)\n",
    "    return results\n",
    "# quiz_model(\"./data/addition.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb9596",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
