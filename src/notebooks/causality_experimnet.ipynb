{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "390b66a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets test forward hooks.\n",
    "# load one SAE\n",
    "# load the model\n",
    "# print attributes/architicture of the sae and the model\n",
    "# feed forward one example to the SAE\n",
    "# get the reconstruction of the activations based on one feature\n",
    "# create a hook that replaces the model's activation with the activation - lambda * reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce9b998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from sae_lens import SAE\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import re\n",
    "# change directory to d:\\\\Master's\\\\gemma_scope_math\\\n",
    "os.chdir(\"d:\\\\Master's\\\\gemma_scope_math\")\n",
    "\n",
    "\n",
    "# Disable gradients for memory efficiency\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af440f4c",
   "metadata": {},
   "source": [
    "# Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd64a2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3714095d58438bb8926eb18ac7d725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    device_map='auto',\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f645d",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e14bb969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(prompt):\n",
    "    \"\"\"\n",
    "    Count the number of tokens in a prompt and return detailed tokenization info.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input string to tokenize\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (token_count, token_strings, token_ids, formatted_string)\n",
    "            - token_count: Number of tokens\n",
    "            - token_strings: List of individual token strings\n",
    "            - token_ids: List of token IDs\n",
    "            - formatted_string: Human-readable representation with token boundaries\n",
    "    \"\"\"\n",
    "    # Tokenize the prompt\n",
    "    tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    # Get individual token strings\n",
    "    token_strings = []\n",
    "    for token_id in tokens:\n",
    "        token_str = tokenizer.decode([token_id])\n",
    "        token_strings.append(token_str)\n",
    "    \n",
    "    # Create a formatted string showing token boundaries\n",
    "    formatted_parts = []\n",
    "    for i, token_str in enumerate(token_strings):\n",
    "        # Add token boundaries and numbering for clarity\n",
    "        formatted_parts.append(f\"[{i}:'{token_str}']\")\n",
    "    \n",
    "    formatted_string = \" \".join(formatted_parts)\n",
    "    \n",
    "    return token_count, token_strings, tokens, formatted_string\n",
    "\n",
    "# Example usage:\n",
    "def demo_tokenization(prompt):\n",
    "    \"\"\"\n",
    "    Demonstrate tokenization for a given prompt.\n",
    "    \"\"\"\n",
    "    count, strings, ids, formatted = count_tokens(prompt)\n",
    "    \n",
    "    print(f\"Original text: '{prompt}'\")\n",
    "    print(f\"Token count: {count}\")\n",
    "    print(f\"Token IDs: {ids}\")\n",
    "    print(f\"Token strings: {strings}\")\n",
    "    print(f\"Formatted view: {formatted}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# count total number of output tokens in the results\n",
    "def count_total_output_tokens(results):\n",
    "    \"\"\"\n",
    "    Count total number of output tokens in the results.\n",
    "    \n",
    "    Args:\n",
    "        results: List of tuples (problem, answer)\n",
    "    \n",
    "    Returns:\n",
    "        int: Total number of output tokens\n",
    "    \"\"\"\n",
    "    total_tokens = 0\n",
    "    for _, answer in results:\n",
    "        # Count tokens in the answer\n",
    "        count, _, _, _ = count_tokens(answer)\n",
    "        total_tokens += count\n",
    "    \n",
    "    return total_tokens\n",
    "\n",
    "def calc_correct_answer(problem):\n",
    "    \"\"\"\n",
    "    Calculate the correct answer for a given problem.\n",
    "    \n",
    "    Args:\n",
    "        problem: Problem string (e.g., \"2 + 2\")\n",
    "    \n",
    "    Returns:\n",
    "        str: Correct answer as a string\n",
    "    \"\"\"\n",
    "    # Remove any non-numeric characters except for +, -, *, /\n",
    "    clean_problem = re.sub(r'[^\\d\\s\\+\\-\\*/]', '', problem)\n",
    "    \n",
    "    try:\n",
    "        # Evaluate the expression safely\n",
    "        answer = eval(clean_problem)\n",
    "        return str(answer)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating problem '{problem}': {e}\")\n",
    "        return \"ERROR\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68cd9c2",
   "metadata": {},
   "source": [
    "## generation utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2326a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generation(prompts, max_new_tokens=10, batch_size=8):\n",
    "    \"\"\"\n",
    "    Generate responses for multiple prompts in batches with improved memory management.\n",
    "    \"\"\"\n",
    "    all_responses = []\n",
    "    \n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch_prompts = prompts[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize batch\n",
    "        inputs = tokenizer(\n",
    "            batch_prompts, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True,\n",
    "            add_special_tokens=True\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                repetition_penalty=1.0,\n",
    "                num_beams=1,  \n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "            \n",
    "            # Move outputs to CPU immediately and decode\n",
    "            outputs_cpu = outputs.cpu()\n",
    "            \n",
    "            # Decode batch responses\n",
    "            batch_responses = []\n",
    "            for j, output in enumerate(outputs_cpu):\n",
    "                # Get only the new tokens (after the input)\n",
    "                new_tokens = output[len(inputs.input_ids[j]):]\n",
    "                response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "                batch_responses.append(response.strip())\n",
    "            \n",
    "            # Clean up GPU tensors\n",
    "            del outputs\n",
    "            del inputs\n",
    "            torch.cuda.empty_cache()  # Force GPU memory cleanup\n",
    "        \n",
    "        all_responses.extend(batch_responses)\n",
    "        \n",
    "        # Optional: print progress\n",
    "        print(f\"Processed {min(i + batch_size, len(prompts))}/{len(prompts)} prompts\")\n",
    "    \n",
    "    return all_responses\n",
    "\n",
    "def batch_quiz_model(dataset_path, max_new_tokens=10, batch_size=8, start_batch=0, end_batch=None, prefix=\"You are a calculator. Answer immediately: \", postfix=\"\"):\n",
    "    \"\"\"\n",
    "    Quiz the model with improved memory management.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        problems = [line.strip() for line in f.readlines() if line.strip()]\n",
    "    \n",
    "    # Calculate total number of batches\n",
    "    total_batches = (len(problems) + batch_size - 1) // batch_size\n",
    "    \n",
    "    # Set end_batch if not specified\n",
    "    if end_batch is None:\n",
    "        end_batch = total_batches\n",
    "    \n",
    "    # Validate batch indices\n",
    "    if start_batch < 0 or start_batch >= total_batches:\n",
    "        raise ValueError(f\"start_batch {start_batch} is out of range [0, {total_batches})\")\n",
    "    if end_batch < start_batch or end_batch > total_batches:\n",
    "        raise ValueError(f\"end_batch {end_batch} is out of range [{start_batch}, {total_batches}]\")\n",
    "    \n",
    "    # Calculate problem indices for the specified batch range\n",
    "    start_idx = start_batch * batch_size\n",
    "    end_idx = min(end_batch * batch_size, len(problems))\n",
    "    \n",
    "    # Get subset of problems for this run\n",
    "    subset_problems = problems[start_idx:end_idx]\n",
    "    \n",
    "    # Add prefix/postfix to problems\n",
    "    prompts = [prefix + problem + postfix for problem in subset_problems]\n",
    "    \n",
    "    print(f\"Processing batches {start_batch} to {end_batch-1} ({len(subset_problems)} problems) in batches of {batch_size}...\")\n",
    "    print(f\"Total dataset size: {len(problems)} problems ({total_batches} total batches)\")\n",
    "    \n",
    "    # Generate answers in batches\n",
    "    answers = batch_generation(prompts, max_new_tokens=max_new_tokens, batch_size=batch_size)\n",
    "    \n",
    "    # Combine problems with answers - store prompts, not the full dataset\n",
    "    results = list(zip(prompts, answers))\n",
    "    \n",
    "    # Clean up large variables\n",
    "    del subset_problems\n",
    "    del prompts\n",
    "    del answers\n",
    "    torch.cuda.empty_cache()  # Force GPU memory cleanup\n",
    "    return results\n",
    "\n",
    "def save_results(results, output_path, append=True):\n",
    "    \"\"\"\n",
    "    Save results to file with option to append or overwrite.\n",
    "    \n",
    "    Args:\n",
    "        results: List of tuples (problem, answer)\n",
    "        output_path: Path to output file\n",
    "        append: If True, append to existing file. If False, overwrite\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    mode = 'a' if append else 'w'\n",
    "    with open(output_path, mode) as f:\n",
    "        for problem, answer in results:\n",
    "            f.write(f\"{problem}\\t{answer}\\n\")\n",
    "    \n",
    "    print(f\"Results {'appended to' if append else 'saved to'} {output_path}\")\n",
    "\n",
    "def find_optimal_batch_size():\n",
    "    \"\"\"Find the largest batch size that fits in GPU memory\"\"\"\n",
    "    batch_size = 256\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Test with your actual inference function\n",
    "            test_prompts = [\"5+7=\"] * batch_size\n",
    "            _ = batch_generation(test_prompts, max_new_tokens=6, batch_size=batch_size)\n",
    "            batch_size *= 2  # Double until we hit memory limit\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            # Use half of the failed size\n",
    "            optimal_size = batch_size // 4  # Go back to last working size\n",
    "            break\n",
    "    \n",
    "    return optimal_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec347c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_correct_answers(dataset_path, batch_size=64, start_batch=0, end_batch=None,):\n",
    "    \"\"\"\n",
    "    Load the same questions as batch_quiz_model and calculate correct answers.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to the dataset file\n",
    "        batch_size: Number of problems to process simultaneously (same as used in batch_quiz_model)\n",
    "        start_batch: Starting batch index (0-based, same as used in batch_quiz_model)\n",
    "        end_batch: Ending batch index (exclusive, same as used in batch_quiz_model)\n",
    "        prefix: Prefix that was added to each problem in batch_quiz_model\n",
    "        postfix: Postfix that was added to each problem in batch_quiz_model\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples (full_prompt, correct_answer)\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        problems = [line.strip() for line in f.readlines() if line.strip()]\n",
    "    \n",
    "    # Calculate total number of batches\n",
    "    total_batches = (len(problems) + batch_size - 1) // batch_size\n",
    "    \n",
    "    # Set end_batch if not specified\n",
    "    if end_batch is None:\n",
    "        end_batch = total_batches\n",
    "    \n",
    "    # Validate batch indices (same validation as batch_quiz_model)\n",
    "    if start_batch < 0 or start_batch >= total_batches:\n",
    "        raise ValueError(f\"start_batch {start_batch} is out of range [0, {total_batches})\")\n",
    "    if end_batch < start_batch or end_batch > total_batches:\n",
    "        raise ValueError(f\"end_batch {end_batch} is out of range [{start_batch}, {total_batches}]\")\n",
    "    \n",
    "    # Calculate problem indices for the specified batch range (same as batch_quiz_model)\n",
    "    start_idx = start_batch * batch_size\n",
    "    end_idx = min(end_batch * batch_size, len(problems))\n",
    "    \n",
    "    # Get subset of problems for this run\n",
    "    subset_problems = problems[start_idx:end_idx]\n",
    "    \n",
    "    results = []\n",
    "    for problem in subset_problems:\n",
    "        # Create the same full prompt as batch_quiz_model does\n",
    "        full_prompt =problem \n",
    "        \n",
    "        # Calculate correct answer for the original problem (without prefix/postfix)\n",
    "        correct_answer = calc_correct_answer(problem)\n",
    "        \n",
    "        results.append((full_prompt, correct_answer))\n",
    "    \n",
    "    print(f\"Calculated correct answers for batches {start_batch} to {end_batch-1} ({len(subset_problems)} problems)\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4ba95d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_first_digits(model_answer, correct_answer):\n",
    "    \"\"\"\n",
    "    Extract the first X digits from the model answer, where X is the length of the correct answer.\n",
    "    Removes any repeated question pattern first and handles step-by-step numbering and negative signs.\n",
    "    \n",
    "    Args:\n",
    "        model_answer: The model's response string\n",
    "        correct_answer: The correct answer as a string\n",
    "    \n",
    "    Returns:\n",
    "        str: First X digits from model answer, or empty string if not enough digits found\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Remove question pattern if it exists (operand1{operation}operand2 with no spaces)\n",
    "    # This pattern matches: digits, operation (+,-,*,/), digits, optional =\n",
    "    question_pattern = r'\\d+[\\+\\-\\*/]\\d+\\s*=?\\s*'\n",
    "    cleaned_answer = re.sub(question_pattern, '', model_answer)\n",
    "    \n",
    "    # Remove step numbering patterns (e.g., \"1.\", \"2.\", etc.)\n",
    "    step_pattern = r'\\b\\d+\\.\\s*'\n",
    "    cleaned_answer = re.sub(step_pattern, '', cleaned_answer)\n",
    "    \n",
    "    # Get the length of the correct answer (including minus sign if present)\n",
    "    target_length = len(correct_answer.strip())\n",
    "    \n",
    "    # Extract the first number (with optional minus sign) from the cleaned model answer\n",
    "    # This pattern matches: optional minus sign followed by digits\n",
    "    number_pattern = r'-?\\d+'\n",
    "    numbers = re.findall(number_pattern, cleaned_answer)\n",
    "    \n",
    "    if numbers:\n",
    "        first_number = numbers[0]\n",
    "        # Truncate to target length if needed\n",
    "        if len(first_number) >= target_length:\n",
    "            return first_number[:target_length]\n",
    "        else:\n",
    "            return first_number\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7334aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model_results, correct_answers):\n",
    "    \"\"\"\n",
    "    Calculate accuracy by comparing extracted numbers from model answers with correct answers.\n",
    "    Handles negative numbers and step-by-step responses.\n",
    "    \n",
    "    Args:\n",
    "        model_results: List of tuples (prompt, model_answer) from batch_quiz_model\n",
    "        correct_answers: List of tuples (prompt, correct_answer) from benchmark_correct_answers\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (accuracy, correct_count, total_count, detailed_results, skipped_count)\n",
    "            - accuracy: Accuracy as a float between 0 and 1\n",
    "            - correct_count: Number of correct answers\n",
    "            - total_count: Total number of valid questions (excluding skipped)\n",
    "            - detailed_results: List of tuples (prompt, correct_answer, model_answer, extracted_number, is_correct)\n",
    "            - skipped_count: Number of prompts skipped due to no valid numbers found\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    if len(model_results) != len(correct_answers):\n",
    "        raise ValueError(f\"Mismatch in result lengths: {len(model_results)} vs {len(correct_answers)}\")\n",
    "    \n",
    "    correct_count = 0\n",
    "    skipped_count = 0\n",
    "    detailed_results = []\n",
    "    \n",
    "    for i, ((model_prompt, model_answer), (correct_prompt, correct_answer)) in enumerate(zip(model_results, correct_answers)):\n",
    "        # Remove question pattern if it exists\n",
    "        question_pattern = r'\\d+[\\+\\-\\*/]\\d+\\s*=?\\s*'\n",
    "        cleaned_answer = re.sub(question_pattern, '', model_answer)\n",
    "        \n",
    "        # Remove step numbering patterns (e.g., \"1.\", \"2.\", etc.)\n",
    "        step_pattern = r'\\b\\d+\\.\\s*'\n",
    "        cleaned_answer = re.sub(step_pattern, '', cleaned_answer)\n",
    "        \n",
    "        # Extract the first number (with optional minus sign) from the cleaned model answer\n",
    "        number_pattern = r'-?\\d+'\n",
    "        numbers = re.findall(number_pattern, cleaned_answer)\n",
    "        \n",
    "        # Skip if no numbers found\n",
    "        if not numbers:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Get the first extracted number\n",
    "        extracted_number = numbers[0]\n",
    "        \n",
    "        # Check if extracted number matches the correct answer\n",
    "        is_correct = extracted_number == correct_answer.strip()\n",
    "        if is_correct:\n",
    "            correct_count += 1\n",
    "        \n",
    "        detailed_results.append((\n",
    "            model_prompt,\n",
    "            correct_answer,\n",
    "            model_answer,\n",
    "            extracted_number,\n",
    "            is_correct\n",
    "        ))\n",
    "    \n",
    "    total_count = len(detailed_results)  # Only count valid results\n",
    "    accuracy = correct_count / total_count if total_count > 0 else 0.0\n",
    "    \n",
    "    return accuracy, correct_count, total_count, detailed_results, skipped_count\n",
    "\n",
    "def print_accuracy_report(accuracy, correct_count, total_count, detailed_results, skipped_count=0, show_errors_only=False, max_examples=10):\n",
    "    \"\"\"\n",
    "    Print a detailed accuracy report.\n",
    "    \n",
    "    Args:\n",
    "        accuracy: Accuracy as a float\n",
    "        correct_count: Number of correct answers\n",
    "        total_count: Total number of questions\n",
    "        detailed_results: Detailed results from calculate_accuracy\n",
    "        skipped_count: Number of prompts skipped due to insufficient digits\n",
    "        show_errors_only: If True, only show incorrect answers\n",
    "        max_examples: Maximum number of examples to show\n",
    "    \"\"\"\n",
    "    print(f\"Accuracy: {accuracy:.3f} ({correct_count}/{total_count})\")\n",
    "    print(f\"Correct: {correct_count}, Incorrect: {total_count - correct_count}\")\n",
    "    if skipped_count > 0:\n",
    "        print(f\"Skipped (insufficient digits): {skipped_count}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    examples_shown = 0\n",
    "    for prompt, correct, model_answer, extracted, is_correct in detailed_results:\n",
    "        if show_errors_only and is_correct:\n",
    "            continue\n",
    "        \n",
    "        if examples_shown >= max_examples:\n",
    "            break\n",
    "        \n",
    "        status = \"✓\" if is_correct else \"✗\"\n",
    "        print(f\"{status} Prompt: {prompt}\")\n",
    "        print(f\"  Correct: {correct}\")\n",
    "        print(f\"  Model output: '{model_answer}'\")\n",
    "        print(f\"  Extracted: '{extracted}'\")\n",
    "        print()\n",
    "        \n",
    "        examples_shown += 1\n",
    "    \n",
    "    if examples_shown < len(detailed_results):\n",
    "        remaining = len(detailed_results) - examples_shown\n",
    "        print(f\"... and {remaining} more examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55c77954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import psutil\n",
    "import shutil\n",
    "\n",
    "# Optional: Use pynvml if you want more structured GPU info\n",
    "try:\n",
    "    from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n",
    "    pynvml_available = True\n",
    "except ImportError:\n",
    "    pynvml_available = False\n",
    "\n",
    "def show_gpu_usage():\n",
    "    print(\"=== GPU ===\")\n",
    "    if pynvml_available:\n",
    "        nvmlInit()\n",
    "        handle = nvmlDeviceGetHandleByIndex(0)  # GPU 0\n",
    "        meminfo = nvmlDeviceGetMemoryInfo(handle)\n",
    "        print(f\"Total: {meminfo.total / 1024**2:.2f} MB\")\n",
    "        print(f\"Used:  {meminfo.used / 1024**2:.2f} MB\")\n",
    "        print(f\"Free:  {meminfo.free / 1024**2:.2f} MB\")\n",
    "    else:\n",
    "        print(subprocess.getoutput(\"nvidia-smi\"))\n",
    "\n",
    "def show_ram_usage():\n",
    "    print(\"\\n=== RAM ===\")\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"Total: {mem.total / 1024**3:.2f} GB\")\n",
    "    print(f\"Used:  {mem.used / 1024**3:.2f} GB\")\n",
    "    print(f\"Free:  {mem.available / 1024**3:.2f} GB\")\n",
    "\n",
    "def show_disk_usage(path=\"/\"):\n",
    "    print(\"\\n=== Disk ===\")\n",
    "    total, used, free = shutil.disk_usage(path)\n",
    "    print(f\"Total: {total / 1024**3:.2f} GB\")\n",
    "    print(f\"Used:  {used / 1024**3:.2f} GB\")\n",
    "    print(f\"Free:  {free / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4443a6",
   "metadata": {},
   "source": [
    "# Benchmark model performnce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50235917",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512  # Set your desired batch size\n",
    "start_batch = 0  # Starting batch index\n",
    "end_batch = 1    # Ending batch index (exclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6357cd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPU ===\n",
      "Total: 6144.00 MB\n",
      "Used:  4206.98 MB\n",
      "Free:  1937.02 MB\n",
      "\n",
      "=== RAM ===\n",
      "Total: 15.85 GB\n",
      "Used:  10.04 GB\n",
      "Free:  5.80 GB\n",
      "\n",
      "=== Disk ===\n",
      "Total: 874.14 GB\n",
      "Used:  635.67 GB\n",
      "Free:  238.47 GB\n"
     ]
    }
   ],
   "source": [
    "show_gpu_usage()\n",
    "show_ram_usage()\n",
    "show_disk_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b0a18ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batches 0 to 0 (512 problems) in batches of 512...\n",
      "Total dataset size: 10000 problems (20 total batches)\n",
      "Processed 512/512 prompts\n",
      "Calculated correct answers for batches 0 to 0 (512 problems)\n",
      "Accuracy: 0.234 (103/441)\n",
      "Correct: 103, Incorrect: 338\n",
      "Skipped (insufficient digits): 71\n",
      "--------------------------------------------------------------------------------\n",
      "✗ Prompt: 6734+7265=\n",
      "  Correct: 13999\n",
      "  Model output: '15000\n",
      "\n",
      "15000-'\n",
      "  Extracted: '15000'\n",
      "\n",
      "✗ Prompt: 1466+5426=\n",
      "  Correct: 6892\n",
      "  Model output: 'Here's how to solve it:\n",
      "\n",
      "**1'\n",
      "  Extracted: '1'\n",
      "\n",
      "✗ Prompt: 6578+9322=\n",
      "  Correct: 15900\n",
      "  Model output: '20000\n",
      "\n",
      "20000-'\n",
      "  Extracted: '20000'\n",
      "\n",
      "✗ Prompt: 7949+3433=\n",
      "  Correct: 11382\n",
      "  Model output: 'Here's how to solve it:\n",
      "\n",
      "**1'\n",
      "  Extracted: '1'\n",
      "\n",
      "✗ Prompt: 7420+2184=\n",
      "  Correct: 9604\n",
      "  Model output: '**Answer:** \n",
      "\n",
      "4604'\n",
      "  Extracted: '4604'\n",
      "\n",
      "✗ Prompt: 7396+9666=\n",
      "  Correct: 17062\n",
      "  Model output: 'Here's how to solve it:\n",
      "\n",
      "**1'\n",
      "  Extracted: '1'\n",
      "\n",
      "✗ Prompt: 3047+3747=\n",
      "  Correct: 6794\n",
      "  Model output: '7794\n",
      "\n",
      "7794/2='\n",
      "  Extracted: '7794'\n",
      "\n",
      "✗ Prompt: 1189+3734=\n",
      "  Correct: 4923\n",
      "  Model output: 'Here's how to solve it:\n",
      "\n",
      "**1'\n",
      "  Extracted: '1'\n",
      "\n",
      "✗ Prompt: 4005+5658=\n",
      "  Correct: 9663\n",
      "  Model output: '10663\n",
      "\n",
      "10663+'\n",
      "  Extracted: '10663'\n",
      "\n",
      "✗ Prompt: 2899+8734=\n",
      "  Correct: 11633\n",
      "  Model output: 'Here's how to solve it:\n",
      "\n",
      "**1'\n",
      "  Extracted: '1'\n",
      "\n",
      "... and 431 more examples\n"
     ]
    }
   ],
   "source": [
    "prefix = \"\"\n",
    "postfix = \"\"\n",
    "results = batch_quiz_model(\"./data/random_addition.txt\", max_new_tokens=12 ,batch_size=batch_size, start_batch=start_batch, end_batch=end_batch, prefix=prefix, postfix=postfix)\n",
    "\n",
    "correct_answers = benchmark_correct_answers(\"./data/random_addition.txt\", batch_size=batch_size, start_batch=start_batch, end_batch=end_batch)\n",
    "\n",
    "# Calculate accuracy with skipping\n",
    "accuracy, correct_count, total_count, detailed_results, skipped_count = calculate_accuracy(results, correct_answers)\n",
    "\n",
    "# Print report including skipped count\n",
    "print_accuracy_report(accuracy, correct_count, total_count, detailed_results, skipped_count, show_errors_only=True, max_examples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67433f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batches 0 to 0 (512 problems) in batches of 512...\n",
      "Total dataset size: 10000 problems (20 total batches)\n",
      "Processed 512/512 prompts\n",
      "Calculated correct answers for batches 0 to 0 (512 problems)\n",
      "Accuracy: 0.851 (406/477)\n",
      "Correct: 406, Incorrect: 71\n",
      "Skipped (insufficient digits): 35\n",
      "--------------------------------------------------------------------------------\n",
      "✗ Prompt: Answer directly: 6734+7265= \n",
      "  Correct: 13999\n",
      "  Model output: '**Answer:** 14000'\n",
      "  Extracted: '14000'\n",
      "\n",
      "✗ Prompt: Answer directly: 6578+9322= \n",
      "  Correct: 15900\n",
      "  Model output: '**Answer:** 15890'\n",
      "  Extracted: '15890'\n",
      "\n",
      "✗ Prompt: Answer directly: 2267+2528= \n",
      "  Correct: 4795\n",
      "  Model output: '**Answer:** 4805'\n",
      "  Extracted: '4805'\n",
      "\n",
      "✗ Prompt: Answer directly: 4943+8555= \n",
      "  Correct: 13498\n",
      "  Model output: '**Answer:** 13508'\n",
      "  Extracted: '13508'\n",
      "\n",
      "✗ Prompt: Answer directly: 1995+8629= \n",
      "  Correct: 10624\n",
      "  Model output: '**Answer:** 10628'\n",
      "  Extracted: '10628'\n",
      "\n",
      "✗ Prompt: Answer directly: 1064+9006= \n",
      "  Correct: 10070\n",
      "  Model output: '**Answer:** 1064 + 9'\n",
      "  Extracted: '1064'\n",
      "\n",
      "✗ Prompt: Answer directly: 6116+7019= \n",
      "  Correct: 13135\n",
      "  Model output: '**Answer:** 13215'\n",
      "  Extracted: '13215'\n",
      "\n",
      "✗ Prompt: Answer directly: 7374+7892= \n",
      "  Correct: 15266\n",
      "  Model output: '**Answer:** 15276'\n",
      "  Extracted: '15276'\n",
      "\n",
      "✗ Prompt: Answer directly: 7776+6864= \n",
      "  Correct: 14640\n",
      "  Model output: '**Answer:** 14630'\n",
      "  Extracted: '14630'\n",
      "\n",
      "✗ Prompt: Answer directly: 5413+4748= \n",
      "  Correct: 10161\n",
      "  Model output: '**Answer:** 10151'\n",
      "  Extracted: '10151'\n",
      "\n",
      "... and 467 more examples\n"
     ]
    }
   ],
   "source": [
    "prefix = \"Answer directly: \"\n",
    "postfix = \" \"\n",
    "results = batch_quiz_model(\"./data/random_addition.txt\", max_new_tokens=12 ,batch_size=batch_size, start_batch=start_batch, end_batch=end_batch, prefix=prefix, postfix=postfix)\n",
    "\n",
    "correct_answers = benchmark_correct_answers(\"./data/random_addition.txt\", batch_size=batch_size, start_batch=start_batch, end_batch=end_batch)\n",
    "\n",
    "# Calculate accuracy with skipping\n",
    "accuracy, correct_count, total_count, detailed_results, skipped_count = calculate_accuracy(results, correct_answers)\n",
    "\n",
    "# Print report including skipped count\n",
    "print_accuracy_report(accuracy, correct_count, total_count, detailed_results, skipped_count, show_errors_only=True, max_examples=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19e664c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPU ===\n",
      "Total: 6144.00 MB\n",
      "Used:  5535.83 MB\n",
      "Free:  608.17 MB\n",
      "\n",
      "=== RAM ===\n",
      "Total: 15.85 GB\n",
      "Used:  11.51 GB\n",
      "Free:  4.33 GB\n",
      "\n",
      "=== Disk ===\n",
      "Total: 874.14 GB\n",
      "Used:  635.67 GB\n",
      "Free:  238.47 GB\n"
     ]
    }
   ],
   "source": [
    "# Run all\n",
    "show_gpu_usage()\n",
    "show_ram_usage()\n",
    "show_disk_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05f5c029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batches 0 to 0 (512 problems) in batches of 512...\n",
      "Total dataset size: 10000 problems (20 total batches)\n",
      "Processed 512/512 prompts\n",
      "Calculated correct answers for batches 0 to 0 (512 problems)\n",
      "Accuracy: 0.341 (155/455)\n",
      "Correct: 155, Incorrect: 300\n",
      "Skipped (insufficient digits): 57\n",
      "--------------------------------------------------------------------------------\n",
      "✗ Prompt: 8270+1860= Answer: \n",
      "  Correct: 10130\n",
      "  Model output: '**Answer:** 31300'\n",
      "  Extracted: '31300'\n",
      "\n",
      "✗ Prompt: 6734+7265= Answer: \n",
      "  Correct: 13999\n",
      "  Model output: '**15000**'\n",
      "  Extracted: '15000'\n",
      "\n",
      "✗ Prompt: 6578+9322= Answer: \n",
      "  Correct: 15900\n",
      "  Model output: '**18890**'\n",
      "  Extracted: '18890'\n",
      "\n",
      "✗ Prompt: 7420+2184= Answer: \n",
      "  Correct: 9604\n",
      "  Model output: '**Solution:**\n",
      "\n",
      "```\n",
      "4604'\n",
      "  Extracted: '4604'\n",
      "\n",
      "✗ Prompt: 7396+9666= Answer: \n",
      "  Correct: 17062\n",
      "  Model output: '**19062**'\n",
      "  Extracted: '19062'\n",
      "\n",
      "✗ Prompt: 3047+3747= Answer: \n",
      "  Correct: 6794\n",
      "  Model output: '**7794**'\n",
      "  Extracted: '7794'\n",
      "\n",
      "✗ Prompt: 2267+2528= Answer: \n",
      "  Correct: 4795\n",
      "  Model output: '**4805**'\n",
      "  Extracted: '4805'\n",
      "\n",
      "✗ Prompt: 9792+9433= Answer: \n",
      "  Correct: 19225\n",
      "  Model output: '**19125**'\n",
      "  Extracted: '19125'\n",
      "\n",
      "✗ Prompt: 8513+3612= Answer: \n",
      "  Correct: 12125\n",
      "  Model output: '**7125**'\n",
      "  Extracted: '7125'\n",
      "\n",
      "✗ Prompt: 6486+8099= Answer: \n",
      "  Correct: 14585\n",
      "  Model output: '**16585**'\n",
      "  Extracted: '16585'\n",
      "\n",
      "... and 445 more examples\n"
     ]
    }
   ],
   "source": [
    "prefix = \"\"\n",
    "postfix = \" Answer: \"\n",
    "results = batch_quiz_model(\"./data/random_addition.txt\", max_new_tokens=12 ,batch_size=batch_size, start_batch=start_batch, end_batch=end_batch, prefix=prefix, postfix=postfix)\n",
    "correct_answers = benchmark_correct_answers(\"./data/random_addition.txt\", batch_size=batch_size, start_batch=start_batch, end_batch=end_batch)\n",
    "\n",
    "# Calculate accuracy with skipping\n",
    "accuracy, correct_count, total_count, detailed_results, skipped_count = calculate_accuracy(results, correct_answers)\n",
    "\n",
    "# Print report including skipped count\n",
    "print_accuracy_report(accuracy, correct_count, total_count, detailed_results, skipped_count, show_errors_only=True, max_examples=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35eccd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPU ===\n",
      "Total: 6144.00 MB\n",
      "Used:  4936.20 MB\n",
      "Free:  1207.80 MB\n",
      "\n",
      "=== RAM ===\n",
      "Total: 15.85 GB\n",
      "Used:  12.11 GB\n",
      "Free:  3.73 GB\n",
      "\n",
      "=== Disk ===\n",
      "Total: 874.14 GB\n",
      "Used:  635.67 GB\n",
      "Free:  238.47 GB\n"
     ]
    }
   ],
   "source": [
    "# Run all\n",
    "show_gpu_usage()\n",
    "show_ram_usage()\n",
    "show_disk_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84d7f94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batches 0 to 0 (512 problems) in batches of 512...\n",
      "Total dataset size: 10000 problems (20 total batches)\n",
      "Processed 512/512 prompts\n",
      "Calculated correct answers for batches 0 to 0 (512 problems)\n",
      "Accuracy: 0.298 (93/312)\n",
      "Correct: 93, Incorrect: 219\n",
      "Skipped (insufficient digits): 200\n",
      "--------------------------------------------------------------------------------\n",
      "✗ Prompt: 6734+7265= \n",
      "  Correct: 13999\n",
      "  Model output: '15000\n",
      "\n",
      "15000-'\n",
      "  Extracted: '15000'\n",
      "\n",
      "✗ Prompt: 7420+2184= \n",
      "  Correct: 9604\n",
      "  Model output: '**Answer:** \n",
      "\n",
      "**4604**'\n",
      "  Extracted: '4604'\n",
      "\n",
      "✗ Prompt: 4005+5658= \n",
      "  Correct: 9663\n",
      "  Model output: '10663\n",
      "\n",
      "10663 is'\n",
      "  Extracted: '10663'\n",
      "\n",
      "✗ Prompt: 2899+8734= \n",
      "  Correct: 11633\n",
      "  Model output: 'Here's how to solve it:\n",
      "\n",
      "**1'\n",
      "  Extracted: '1'\n",
      "\n",
      "✗ Prompt: 9838+6393= \n",
      "  Correct: 16231\n",
      "  Model output: '**Answer:** \n",
      "\n",
      "**13231'\n",
      "  Extracted: '13231'\n",
      "\n",
      "✗ Prompt: 8041+7235= \n",
      "  Correct: 15276\n",
      "  Model output: '14280\n",
      "\n",
      "14280-'\n",
      "  Extracted: '14280'\n",
      "\n",
      "✗ Prompt: 6486+8099= \n",
      "  Correct: 14585\n",
      "  Model output: '16585\n",
      "\n",
      "16585 is'\n",
      "  Extracted: '16585'\n",
      "\n",
      "✗ Prompt: 1775+9226= \n",
      "  Correct: 11001\n",
      "  Model output: 'Here's how to solve it:\n",
      "\n",
      "**1'\n",
      "  Extracted: '1'\n",
      "\n",
      "✗ Prompt: 4943+8555= \n",
      "  Correct: 13498\n",
      "  Model output: '14500\n",
      "\n",
      "14500-'\n",
      "  Extracted: '14500'\n",
      "\n",
      "✗ Prompt: 4073+2021= \n",
      "  Correct: 6094\n",
      "  Model output: '4073+2021\n",
      "\n",
      "40'\n",
      "  Extracted: '40'\n",
      "\n",
      "... and 302 more examples\n"
     ]
    }
   ],
   "source": [
    "prefix = \"\"\n",
    "postfix = \" \"\n",
    "results = batch_quiz_model(\"./data/random_addition.txt\", max_new_tokens=12 ,batch_size=batch_size, start_batch=start_batch, end_batch=end_batch, prefix=prefix, postfix=postfix)\n",
    "correct_answers = benchmark_correct_answers(\"./data/random_addition.txt\", batch_size=batch_size, start_batch=start_batch, end_batch=end_batch)\n",
    "\n",
    "# Calculate accuracy with skipping\n",
    "accuracy, correct_count, total_count, detailed_results, skipped_count = calculate_accuracy(results, correct_answers)\n",
    "\n",
    "# Print report including skipped count\n",
    "print_accuracy_report(accuracy, correct_count, total_count, detailed_results, skipped_count, show_errors_only=True, max_examples=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81ae6a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batches 0 to 0 (512 problems) in batches of 512...\n",
      "Total dataset size: 10000 problems (20 total batches)\n",
      "Processed 512/512 prompts\n",
      "Calculated correct answers for batches 0 to 0 (512 problems)\n",
      "Accuracy: 0.419 (184/439)\n",
      "Correct: 184, Incorrect: 255\n",
      "Skipped (insufficient digits): 73\n",
      "--------------------------------------------------------------------------------\n",
      "✗ Prompt: 8270+1860= final answer: \n",
      "  Correct: 10130\n",
      "  Model output: '**Answer:** 31300'\n",
      "  Extracted: '31300'\n",
      "\n",
      "✗ Prompt: 6734+7265= final answer: \n",
      "  Correct: 13999\n",
      "  Model output: '14999\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "You simply'\n",
      "  Extracted: '14999'\n",
      "\n",
      "✗ Prompt: 6578+9322= final answer: \n",
      "  Correct: 15900\n",
      "  Model output: '16890\n",
      "\n",
      "Here's how to solve'\n",
      "  Extracted: '16890'\n",
      "\n",
      "✗ Prompt: 7949+3433= final answer: \n",
      "  Correct: 11382\n",
      "  Model output: '**Answer:** 7382'\n",
      "  Extracted: '7382'\n",
      "\n",
      "✗ Prompt: 7420+2184= final answer: \n",
      "  Correct: 9604\n",
      "  Model output: '**Answer:** 4604'\n",
      "  Extracted: '4604'\n",
      "\n",
      "✗ Prompt: 7396+9666= final answer: \n",
      "  Correct: 17062\n",
      "  Model output: '**19062**'\n",
      "  Extracted: '19062'\n",
      "\n",
      "✗ Prompt: 9838+6393= final answer: \n",
      "  Correct: 16231\n",
      "  Model output: '**13231**'\n",
      "  Extracted: '13231'\n",
      "\n",
      "✗ Prompt: 8513+3612= final answer: \n",
      "  Correct: 12125\n",
      "  Model output: '**Answer:** 7125\n",
      "\n",
      "**Explanation'\n",
      "  Extracted: '7125'\n",
      "\n",
      "✗ Prompt: 8041+7235= final answer: \n",
      "  Correct: 15276\n",
      "  Model output: '**14270**'\n",
      "  Extracted: '14270'\n",
      "\n",
      "✗ Prompt: 6486+8099= final answer: \n",
      "  Correct: 14585\n",
      "  Model output: '16585\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "You simply'\n",
      "  Extracted: '16585'\n",
      "\n",
      "... and 429 more examples\n"
     ]
    }
   ],
   "source": [
    "prefix = \"\"\n",
    "postfix = \" final answer: \"\n",
    "results = batch_quiz_model(\"./data/random_addition.txt\", max_new_tokens=12 ,batch_size=batch_size, start_batch=start_batch, end_batch=end_batch, prefix=prefix, postfix=postfix)\n",
    "\n",
    "correct_answers = benchmark_correct_answers(\"./data/random_addition.txt\", batch_size=batch_size, start_batch=start_batch, end_batch=end_batch)\n",
    "\n",
    "# Calculate accuracy with skipping\n",
    "accuracy, correct_count, total_count, detailed_results, skipped_count = calculate_accuracy(results, correct_answers)\n",
    "\n",
    "# Print report including skipped count\n",
    "print_accuracy_report(accuracy, correct_count, total_count, detailed_results, skipped_count, show_errors_only=True, max_examples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59130ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batches 0 to 0 (512 problems) in batches of 512...\n",
      "Total dataset size: 10000 problems (20 total batches)\n",
      "Processed 512/512 prompts\n",
      "Calculated correct answers for batches 0 to 0 (512 problems)\n",
      "Accuracy: 0.986 (492/499)\n",
      "Correct: 492, Incorrect: 7\n",
      "Skipped (insufficient digits): 13\n",
      "--------------------------------------------------------------------------------\n",
      "✗ Prompt: Answer directly: 0+29= \n",
      "  Correct: 29\n",
      "  Model output: 'Answer: 30'\n",
      "  Extracted: '30'\n",
      "\n",
      "✗ Prompt: Answer directly: 0+39= \n",
      "  Correct: 39\n",
      "  Model output: 'Answer: 40'\n",
      "  Extracted: '40'\n",
      "\n",
      "✗ Prompt: Answer directly: 0+49= \n",
      "  Correct: 49\n",
      "  Model output: '**Answer:** 50'\n",
      "  Extracted: '50'\n",
      "\n",
      "✗ Prompt: Answer directly: 0+59= \n",
      "  Correct: 59\n",
      "  Model output: 'Answer: 60'\n",
      "  Extracted: '60'\n",
      "\n",
      "✗ Prompt: Answer directly: 0+79= \n",
      "  Correct: 79\n",
      "  Model output: '**Answer:** 80'\n",
      "  Extracted: '80'\n",
      "\n",
      "✗ Prompt: Answer directly: 0+89= \n",
      "  Correct: 89\n",
      "  Model output: '**Answer:** 99'\n",
      "  Extracted: '99'\n",
      "\n",
      "✗ Prompt: Answer directly: 0+99= \n",
      "  Correct: 99\n",
      "  Model output: '**Answer:** 100'\n",
      "  Extracted: '100'\n",
      "\n",
      "... and 492 more examples\n"
     ]
    }
   ],
   "source": [
    "prefix = \"Answer directly: \"\n",
    "postfix = \" \"\n",
    "results = batch_quiz_model(\"./data/addition.txt\", max_new_tokens=8 ,batch_size=batch_size, start_batch=start_batch, end_batch=end_batch, prefix=prefix, postfix=postfix)\n",
    "\n",
    "correct_answers = benchmark_correct_answers(\"./data/addition.txt\", batch_size=batch_size, start_batch=start_batch, end_batch=end_batch)\n",
    "\n",
    "# Calculate accuracy with skipping\n",
    "accuracy, correct_count, total_count, detailed_results, skipped_count = calculate_accuracy(results, correct_answers)\n",
    "\n",
    "# Print report including skipped count\n",
    "print_accuracy_report(accuracy, correct_count, total_count, detailed_results, skipped_count, show_errors_only=True, max_examples=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b28b82a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batches 0 to 0 (512 problems) in batches of 512...\n",
      "Total dataset size: 10000 problems (20 total batches)\n",
      "Processed 512/512 prompts\n",
      "Calculated correct answers for batches 0 to 0 (512 problems)\n",
      "Accuracy: 0.462 (163/353)\n",
      "Correct: 163, Incorrect: 190\n",
      "Skipped (insufficient digits): 159\n",
      "--------------------------------------------------------------------------------\n",
      "✗ Prompt: Answer directly: 0-8= \n",
      "  Correct: -8\n",
      "  Model output: '**Answer:** 8'\n",
      "  Extracted: '8'\n",
      "\n",
      "✗ Prompt: Answer directly: 0-15= \n",
      "  Correct: -15\n",
      "  Model output: '**Answer:** 15'\n",
      "  Extracted: '15'\n",
      "\n",
      "✗ Prompt: Answer directly: 0-18= \n",
      "  Correct: -18\n",
      "  Model output: '**Answer:** 18'\n",
      "  Extracted: '18'\n",
      "\n",
      "✗ Prompt: Answer directly: 0-20= \n",
      "  Correct: -20\n",
      "  Model output: '**Answer:** 20'\n",
      "  Extracted: '20'\n",
      "\n",
      "✗ Prompt: Answer directly: 0-21= \n",
      "  Correct: -21\n",
      "  Model output: '**Answer:** 0-21 = 21'\n",
      "  Extracted: '21'\n",
      "\n",
      "✗ Prompt: Answer directly: 0-23= \n",
      "  Correct: -23\n",
      "  Model output: '**Answer:** \n",
      "-24'\n",
      "  Extracted: '-24'\n",
      "\n",
      "✗ Prompt: Answer directly: 0-24= \n",
      "  Correct: -24\n",
      "  Model output: '**Answer:** 24'\n",
      "  Extracted: '24'\n",
      "\n",
      "✗ Prompt: Answer directly: 0-25= \n",
      "  Correct: -25\n",
      "  Model output: '**Answer:** 25'\n",
      "  Extracted: '25'\n",
      "\n",
      "✗ Prompt: Answer directly: 0-28= \n",
      "  Correct: -28\n",
      "  Model output: '**Answer:** 28'\n",
      "  Extracted: '28'\n",
      "\n",
      "✗ Prompt: Answer directly: 0-29= \n",
      "  Correct: -29\n",
      "  Model output: '**Answer:** 29'\n",
      "  Extracted: '29'\n",
      "\n",
      "... and 343 more examples\n"
     ]
    }
   ],
   "source": [
    "prefix = \"Answer directly: \"\n",
    "postfix = \" \"\n",
    "dataset = \"./data/subtraction.txt\"\n",
    "results = batch_quiz_model(dataset, max_new_tokens=20, batch_size=batch_size, start_batch=start_batch, end_batch=end_batch, prefix=prefix, postfix=postfix)\n",
    "\n",
    "correct_answers = benchmark_correct_answers(dataset, batch_size=batch_size, start_batch=start_batch, end_batch=end_batch)\n",
    "\n",
    "# Calculate accuracy with skipping\n",
    "accuracy, correct_count, total_count, detailed_results, skipped_count = calculate_accuracy(results, correct_answers)\n",
    "\n",
    "# Print report including skipped count\n",
    "print_accuracy_report(accuracy, correct_count, total_count, detailed_results, skipped_count, show_errors_only=True, max_examples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af9e17bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPU ===\n",
      "Total: 6144.00 MB\n",
      "Used:  4342.87 MB\n",
      "Free:  1801.13 MB\n",
      "\n",
      "=== RAM ===\n",
      "Total: 15.85 GB\n",
      "Used:  12.35 GB\n",
      "Free:  3.49 GB\n",
      "\n",
      "=== Disk ===\n",
      "Total: 874.14 GB\n",
      "Used:  635.67 GB\n",
      "Free:  238.47 GB\n"
     ]
    }
   ],
   "source": [
    "# Run all\n",
    "show_gpu_usage()\n",
    "show_ram_usage()\n",
    "show_disk_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e98d8971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batches 0 to 0 (512 problems) in batches of 512...\n",
      "Total dataset size: 10000 problems (20 total batches)\n",
      "Processed 512/512 prompts\n",
      "Calculated correct answers for batches 0 to 0 (512 problems)\n",
      "Accuracy: 0.766 (353/461)\n",
      "Correct: 353, Incorrect: 108\n",
      "Skipped (insufficient digits): 51\n",
      "--------------------------------------------------------------------------------\n",
      "✗ Prompt: Answer directly: 6390-6191= \n",
      "  Correct: 199\n",
      "  Model output: '200\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "You're asking for a direct answer, and I can provide that.'\n",
      "  Extracted: '200'\n",
      "\n",
      "✗ Prompt: Answer directly: 6734-7265= \n",
      "  Correct: -531\n",
      "  Model output: '**Answer:** -5311'\n",
      "  Extracted: '-5311'\n",
      "\n",
      "✗ Prompt: Answer directly: 3568-6463= \n",
      "  Correct: -2895\n",
      "  Model output: '**Answer:**  -995'\n",
      "  Extracted: '-995'\n",
      "\n",
      "✗ Prompt: Answer directly: 3027-3695= \n",
      "  Correct: -668\n",
      "  Model output: '**Answer:** -628'\n",
      "  Extracted: '-628'\n",
      "\n",
      "✗ Prompt: Answer directly: 7278-9392= \n",
      "  Correct: -2114\n",
      "  Model output: '**Answer:** -2104'\n",
      "  Extracted: '-2104'\n",
      "\n",
      "✗ Prompt: Answer directly: 3454-9996= \n",
      "  Correct: -6542\n",
      "  Model output: '**Answer:** -6512'\n",
      "  Extracted: '-6512'\n",
      "\n",
      "✗ Prompt: Answer directly: 6056-9110= \n",
      "  Correct: -3054\n",
      "  Model output: 'Please provide the answer and an explanation. \n",
      "\n",
      "\n",
      "**Answer:** -3044\n",
      "\n",
      "**Explanation:**'\n",
      "  Extracted: '-3044'\n",
      "\n",
      "✗ Prompt: Answer directly: 8721-8035= \n",
      "  Correct: 686\n",
      "  Model output: '676\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "You can solve this directly by subtracting the numbers:\n",
      "\n",
      "8721 -'\n",
      "  Extracted: '676'\n",
      "\n",
      "✗ Prompt: Answer directly: 1863-3790= \n",
      "  Correct: -1927\n",
      "  Model output: '**Answer:** -1914'\n",
      "  Extracted: '-1914'\n",
      "\n",
      "✗ Prompt: Answer directly: 8408-9755= \n",
      "  Correct: -1347\n",
      "  Model output: 'Please provide the answer and an explanation. \n",
      "\n",
      "\n",
      "**Answer:** -3147\n",
      "\n",
      "**Explanation:**'\n",
      "  Extracted: '-3147'\n",
      "\n",
      "... and 451 more examples\n"
     ]
    }
   ],
   "source": [
    "prefix = \"Answer directly: \"\n",
    "postfix = \" \"\n",
    "dataset = \"./data/random_subtraction.txt\"\n",
    "results = batch_quiz_model(dataset, max_new_tokens=25, batch_size=batch_size, start_batch=start_batch, end_batch=end_batch, prefix=prefix, postfix=postfix)\n",
    "\n",
    "correct_answers = benchmark_correct_answers(dataset, batch_size=batch_size, start_batch=start_batch, end_batch=end_batch)\n",
    "\n",
    "# Calculate accuracy with skipping\n",
    "accuracy, correct_count, total_count, detailed_results, skipped_count = calculate_accuracy(results, correct_answers)\n",
    "\n",
    "# Print report including skipped count\n",
    "print_accuracy_report(accuracy, correct_count, total_count, detailed_results, skipped_count, show_errors_only=True, max_examples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca06f206",
   "metadata": {},
   "source": [
    "# Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b1020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Modification Hook\n",
    "class AblationHook:\n",
    "    def __init__(self, layer_idx, lambda_scale,):\n",
    "        \"\"\"\n",
    "        Initialize the ablation hook.\n",
    "        \n",
    "        Args:\n",
    "            sae: Sparse Autoencoder instance\n",
    "            lambda_scale: Scaling factor for the single latent reconstruction\n",
    "        \"\"\"\n",
    "        # Initialize SAE for layer 12\n",
    "        sae_id = f\"layer_{layer_idx}/width_16k/canonical\"\n",
    "        sae, _, _ = SAE.from_pretrained(\n",
    "            release=\"gemma-scope-2b-pt-res-canonical\",\n",
    "            sae_id=sae_id\n",
    "        )\n",
    "\n",
    "        model_dtype = next(model.parameters()).dtype\n",
    "        sae = sae.to(device=device, dtype=model_dtype)\n",
    "        sae.eval()\n",
    "\n",
    "        self.sae = sae\n",
    "        self.lambda_scale = lambda_scale\n",
    "        self.sae_hook_handles = []\n",
    "    \n",
    "\n",
    "    # Single Latent Reconstruction Hook\n",
    "    def single_latent_hook(self, module, input, output, latent_idx):\n",
    "        \"\"\"\n",
    "        Forward hook to zero all latent activations except for the specified index.\n",
    "        \n",
    "        Args:\n",
    "            module: The HookPoint module (hook_sae_acts_post)\n",
    "            input: Input to the hook (not used here)\n",
    "            output: feature_acts tensor of shape [..., d_sae]\n",
    "            latent_idx: Index of the latent to keep non-zero\n",
    "        \n",
    "        Returns:\n",
    "            Modified feature_acts with only latent_idx non-zero\n",
    "        \"\"\"\n",
    "        modified_acts = torch.zeros_like(output)\n",
    "        modified_acts[..., latent_idx] = output[..., latent_idx]\n",
    "        # validate that the modified acts at the latent index are the same as the no ablation case. \n",
    "        # print(f\"Modified acts at latent index {latent_idx}: {modified_acts[..., latent_idx]}\")\n",
    "\n",
    "        return modified_acts # TODO: DO I need to use an activation function here?\n",
    "    \n",
    "\n",
    "    # Register the activation modification hook\n",
    "    def register_hook(self, latent_idx):\n",
    "        # Register hook to capture single latent reconstruction\n",
    "        self.sae_hook_handles.append(self.sae.hook_sae_acts_post.register_forward_hook(\n",
    "            lambda m, i, o: self.single_latent_hook(m, i, o, latent_idx=latent_idx)\n",
    "        ))\n",
    "\n",
    "    def unregister_hook(self):\n",
    "        if hasattr(self, 'sae_hook_handles'):\n",
    "            for hook_handle in self.sae_hook_handles:\n",
    "                hook_handle.remove()\n",
    "        self.sae_hook_handles = []\n",
    "\n",
    "    # def run_hooked_sae(self, hidden_states):\n",
    "    def activation_modification_hook(self, module, input, output):\n",
    "        hidden_states = output[0]\n",
    "        \n",
    "        # Compute single latent reconstruction on current hidden_states\n",
    "        with torch.no_grad():\n",
    "                reconstruction_w_hooks = self.sae(hidden_states)\n",
    "        \n",
    "        modified_hidden_states = hidden_states - self.lambda_scale * reconstruction_w_hooks\n",
    "        \n",
    "        return modified_hidden_states # TODO: check that this has the correct shape, value, and dtype. \n",
    "\n",
    "    def run_encoding_decoding(self, hidden_states):\n",
    "        \"\"\"\n",
    "        Run encoding and Calculate the reconstruction loss for the given hidden states.\n",
    "        \n",
    "        Args:\n",
    "            hidden_states: The hidden states to calculate the loss for\n",
    "            \n",
    "        Returns:\n",
    "            The reconstruction loss\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            latents = self.sae.encode(hidden_states)\n",
    "            # Compute single latent reconstruction on current hidden_states\n",
    "            reconstruction = self.sae(hidden_states)\n",
    "            # Calculate the mean squared error loss\n",
    "            loss = F.mse_loss(reconstruction, hidden_states)\n",
    "        return reconstruction, latents, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2efad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment settings\n",
    "import os\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
    "os.environ[\"USE_TRITON\"] = \"0\"\n",
    "layer_idx = 12  # Layer to modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ba471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"5+7= \"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(device)\n",
    "print(f\"Input tokens: {inputs}\")\n",
    "\n",
    "# Get original hidden states\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs, output_hidden_states=True)\n",
    "# print(outputs.shape)\n",
    "print(len(outputs.hidden_states))\n",
    "# original_hidden_states = outputs.hidden_states[layer_idx, -1, :]\n",
    "# print(f\"Original hidden states shape: {original_hidden_states.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecca2d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(outputs.hidden_states[layer_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70157956",
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_hook = AblationHook(layer_idx=12, lambda_scale = 1.0,)\n",
    "reconstruction, latents, loss = ablation_hook.run_encoding_decoding(original_hidden_states)\n",
    "ablation_hook.register_hook(latent_idx=11301)\n",
    "ablation_reconstruction, ablation_latents, ablation_loss = ablation_hook.run_encoding_decoding(original_hidden_states)\n",
    "\n",
    "print(reconstruction.shape)\n",
    "print(f\"Reconstruction loss: {loss.item()}\")\n",
    "print(f\"Ablation reconstruction shape: {ablation_reconstruction.shape}\")\n",
    "print(f\"Ablation reconstruction loss: {ablation_loss.item()}\")\n",
    "print(f\"Latents shape: {latents.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d0c0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents[0,:, 11301] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295d4209",
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_latents[0,:, 11301]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584bb779",
   "metadata": {},
   "source": [
    "## Quick check: are latents zero for all tokens except the first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9383dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print current dir\n",
    "import os\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad40477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "latents_npz_path = r\"../../latents/addition/layer_12.npz\"\n",
    "latents_npz = sparse.load_npz(latents_npz_path)\n",
    "print(latents_npz.shape)  # List all arrays in the npz file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3ebd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latents_npz[:,11301].toarray())  # Print the latents for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a89644c",
   "metadata": {},
   "source": [
    "# Misc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766e0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_generation(prompt, max_new_tokens=12):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(device)\n",
    "    print(f\"Input tokens: {inputs}\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=max_new_tokens,              # Limit tokens for just the number\n",
    "            do_sample=False,                # Deterministic output\n",
    "            repetition_penalty=1.0,         # Avoid penalizing repeated digits\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            num_beams=1,                  # Single beam for simplicity\n",
    "            # # Stop at common continuation tokens\n",
    "            # bad_words_ids=[[tokenizer.encode(\" The\")[0]], \n",
    "            #               [tokenizer.encode(\" So\")[0]],\n",
    "            #               [tokenizer.encode(\"\\n\")[0]],\n",
    "            #               [tokenizer.encode(\"Here\")[0]],\n",
    "            #               ]\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
    "    # Extract just the number\n",
    "    return response.strip()  # Assuming the model outputs just the number\n",
    "    # number_match = re.search(r'^\\s*(\\d+)', response.strip())\n",
    "    # return number_match.group(1) if number_match else response.strip()\n",
    "    \n",
    "def quiz_model(dataset: str):\n",
    "    \"\"\"\n",
    "    Quiz the model on a dataset of arithmetic problems.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    with open(dataset, 'r') as f:\n",
    "        problems = f.readlines()\n",
    "    \n",
    "    results = []\n",
    "    for problem in problems:\n",
    "        problem = problem.strip()\n",
    "        problem = \"You are a calculator. Answer immediately: \" + problem\n",
    "        if problem:\n",
    "            answer = quick_generation(problem, max_new_tokens=10)\n",
    "            results.append((problem, answer))\n",
    "            print(answer)\n",
    "    return results\n",
    "# quiz_model(\"./data/addition.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb9596",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
